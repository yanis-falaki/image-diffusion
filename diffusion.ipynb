{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import UNetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "\n",
    "def quadratic_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    # Generate a linear space from 0 to 1\n",
    "    linear_space = torch.linspace(0, 1, timesteps)\n",
    "    \n",
    "    # Apply a quadratic transformation\n",
    "    quadratic_space = linear_space ** 2\n",
    "    \n",
    "    # Scale and shift the quadratic space to start and end at the specified values\n",
    "    beta_values = start + (end - start) * quadratic_space\n",
    "    \n",
    "    return beta_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 2000\n",
    "\n",
    "# Define beta schedule\n",
    "T = 250\n",
    "betas = quadratic_beta_schedule(timesteps=T).to(device)\n",
    "\n",
    "# Pre-calculate different terms for closed form\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0/ alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transformed_dataset():\n",
    "    data_transforms = [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(), # Scales data into [0, 1]\n",
    "        transforms.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1]\n",
    "    ]\n",
    "\n",
    "    data_transform = transforms.Compose(data_transforms)\n",
    "\n",
    "    train = torchvision.datasets.FGVCAircraft(root=\".\", download=True, transform=data_transform)\n",
    "\n",
    "    test = torchvision.datasets.FGVCAircraft(root=\".\", download=True, transform=data_transform, split='test')\n",
    "\n",
    "    return torch.utils.data.ConcatDataset([train, test])\n",
    "\n",
    "def show_tensor_image(image):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "        transforms.Lambda(lambda t: t * 255.),\n",
    "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "\n",
    "    # take first image of batch\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0, :, :, :]\n",
    "    plt.imshow(reverse_transforms(image))\n",
    "\n",
    "data = load_transformed_dataset()\n",
    "dataloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetv2.GoodUNet(32).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(weight_decay=1e-4, params=model.parameters(), lr=0.05)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32602467"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000], Step [1/156], Loss: 1.119\n",
      "Epoch [1/2000], Step [2/156], Loss: 2.380\n",
      "Epoch [1/2000], Step [3/156], Loss: 1.250\n",
      "Epoch [1/2000], Step [4/156], Loss: 1.032\n",
      "Epoch [1/2000], Step [5/156], Loss: 1.121\n",
      "Epoch [1/2000], Step [6/156], Loss: 1.185\n",
      "Epoch [1/2000], Step [7/156], Loss: 1.145\n",
      "Epoch [1/2000], Step [8/156], Loss: 1.077\n",
      "Epoch [1/2000], Step [9/156], Loss: 1.034\n",
      "Epoch [1/2000], Step [10/156], Loss: 1.028\n",
      "Epoch [1/2000], Step [11/156], Loss: 1.036\n",
      "Epoch [1/2000], Step [12/156], Loss: 1.030\n",
      "Epoch [1/2000], Step [13/156], Loss: 1.024\n",
      "Epoch [1/2000], Step [14/156], Loss: 1.016\n",
      "Epoch [1/2000], Step [15/156], Loss: 1.014\n",
      "Epoch [1/2000], Step [16/156], Loss: 1.019\n",
      "Epoch [1/2000], Step [17/156], Loss: 1.020\n",
      "Epoch [1/2000], Step [18/156], Loss: 1.020\n",
      "Epoch [1/2000], Step [19/156], Loss: 1.018\n",
      "Epoch [1/2000], Step [20/156], Loss: 1.009\n",
      "Epoch [1/2000], Step [21/156], Loss: 1.004\n",
      "Epoch [1/2000], Step [22/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [23/156], Loss: 1.005\n",
      "Epoch [1/2000], Step [24/156], Loss: 1.004\n",
      "Epoch [1/2000], Step [25/156], Loss: 1.007\n",
      "Epoch [1/2000], Step [26/156], Loss: 1.009\n",
      "Epoch [1/2000], Step [27/156], Loss: 1.005\n",
      "Epoch [1/2000], Step [28/156], Loss: 1.005\n",
      "Epoch [1/2000], Step [29/156], Loss: 1.003\n",
      "Epoch [1/2000], Step [30/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [31/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [32/156], Loss: 1.003\n",
      "Epoch [1/2000], Step [33/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [34/156], Loss: 1.003\n",
      "Epoch [1/2000], Step [35/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [36/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [37/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [38/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [39/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [40/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [41/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [42/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [43/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [44/156], Loss: 0.998\n",
      "Epoch [1/2000], Step [45/156], Loss: 1.003\n",
      "Epoch [1/2000], Step [46/156], Loss: 1.003\n",
      "Epoch [1/2000], Step [47/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [48/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [49/156], Loss: 1.003\n",
      "Epoch [1/2000], Step [50/156], Loss: 1.003\n",
      "Epoch [1/2000], Step [51/156], Loss: 1.003\n",
      "Epoch [1/2000], Step [52/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [53/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [54/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [55/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [56/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [57/156], Loss: 1.003\n",
      "Epoch [1/2000], Step [58/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [59/156], Loss: 1.004\n",
      "Epoch [1/2000], Step [60/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [61/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [62/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [63/156], Loss: 0.998\n",
      "Epoch [1/2000], Step [64/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [65/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [66/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [67/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [68/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [69/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [70/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [71/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [72/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [73/156], Loss: 0.997\n",
      "Epoch [1/2000], Step [74/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [75/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [76/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [77/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [78/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [79/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [80/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [81/156], Loss: 0.998\n",
      "Epoch [1/2000], Step [82/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [83/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [84/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [85/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [86/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [87/156], Loss: 0.998\n",
      "Epoch [1/2000], Step [88/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [89/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [90/156], Loss: 0.998\n",
      "Epoch [1/2000], Step [91/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [92/156], Loss: 1.003\n",
      "Epoch [1/2000], Step [93/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [94/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [95/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [96/156], Loss: 0.998\n",
      "Epoch [1/2000], Step [97/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [98/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [99/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [100/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [101/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [102/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [103/156], Loss: 0.998\n",
      "Epoch [1/2000], Step [104/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [105/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [106/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [107/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [108/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [109/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [110/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [111/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [112/156], Loss: 1.003\n",
      "Epoch [1/2000], Step [113/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [114/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [115/156], Loss: 1.004\n",
      "Epoch [1/2000], Step [116/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [117/156], Loss: 0.998\n",
      "Epoch [1/2000], Step [118/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [119/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [120/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [121/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [122/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [123/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [124/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [125/156], Loss: 0.998\n",
      "Epoch [1/2000], Step [126/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [127/156], Loss: 0.997\n",
      "Epoch [1/2000], Step [128/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [129/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [130/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [131/156], Loss: 1.003\n",
      "Epoch [1/2000], Step [132/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [133/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [134/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [135/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [136/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [137/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [138/156], Loss: 0.998\n",
      "Epoch [1/2000], Step [139/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [140/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [141/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [142/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [143/156], Loss: 1.002\n",
      "Epoch [1/2000], Step [144/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [145/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [146/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [147/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [148/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [149/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [150/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [151/156], Loss: 1.001\n",
      "Epoch [1/2000], Step [152/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [153/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [154/156], Loss: 1.000\n",
      "Epoch [1/2000], Step [155/156], Loss: 0.999\n",
      "Epoch [1/2000], Step [156/156], Loss: 1.000\n",
      "Epoch [1/2000], Accumulated_Loss: 1.017\n",
      "Epoch [2/2000], Step [1/156], Loss: 1.002\n",
      "Epoch [2/2000], Step [2/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [3/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [4/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [5/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [6/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [7/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [8/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [9/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [10/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [11/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [12/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [13/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [14/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [15/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [16/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [17/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [18/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [19/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [20/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [21/156], Loss: 1.003\n",
      "Epoch [2/2000], Step [22/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [23/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [24/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [25/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [26/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [27/156], Loss: 0.997\n",
      "Epoch [2/2000], Step [28/156], Loss: 1.003\n",
      "Epoch [2/2000], Step [29/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [30/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [31/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [32/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [33/156], Loss: 1.004\n",
      "Epoch [2/2000], Step [34/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [35/156], Loss: 0.997\n",
      "Epoch [2/2000], Step [36/156], Loss: 1.003\n",
      "Epoch [2/2000], Step [37/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [38/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [39/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [40/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [41/156], Loss: 1.002\n",
      "Epoch [2/2000], Step [42/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [43/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [44/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [45/156], Loss: 1.003\n",
      "Epoch [2/2000], Step [46/156], Loss: 1.003\n",
      "Epoch [2/2000], Step [47/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [48/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [49/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [50/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [51/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [52/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [53/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [54/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [55/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [56/156], Loss: 0.996\n",
      "Epoch [2/2000], Step [57/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [58/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [59/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [60/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [61/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [62/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [63/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [64/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [65/156], Loss: 1.002\n",
      "Epoch [2/2000], Step [66/156], Loss: 1.002\n",
      "Epoch [2/2000], Step [67/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [68/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [69/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [70/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [71/156], Loss: 0.996\n",
      "Epoch [2/2000], Step [72/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [73/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [74/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [75/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [76/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [77/156], Loss: 0.997\n",
      "Epoch [2/2000], Step [78/156], Loss: 1.002\n",
      "Epoch [2/2000], Step [79/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [80/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [81/156], Loss: 1.003\n",
      "Epoch [2/2000], Step [82/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [83/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [84/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [85/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [86/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [87/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [88/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [89/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [90/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [91/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [92/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [93/156], Loss: 0.997\n",
      "Epoch [2/2000], Step [94/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [95/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [96/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [97/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [98/156], Loss: 1.004\n",
      "Epoch [2/2000], Step [99/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [100/156], Loss: 1.002\n",
      "Epoch [2/2000], Step [101/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [102/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [103/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [104/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [105/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [106/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [107/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [108/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [109/156], Loss: 0.997\n",
      "Epoch [2/2000], Step [110/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [111/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [112/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [113/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [114/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [115/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [116/156], Loss: 1.002\n",
      "Epoch [2/2000], Step [117/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [118/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [119/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [120/156], Loss: 1.002\n",
      "Epoch [2/2000], Step [121/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [122/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [123/156], Loss: 1.002\n",
      "Epoch [2/2000], Step [124/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [125/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [126/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [127/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [128/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [129/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [130/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [131/156], Loss: 1.002\n",
      "Epoch [2/2000], Step [132/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [133/156], Loss: 1.003\n",
      "Epoch [2/2000], Step [134/156], Loss: 0.997\n",
      "Epoch [2/2000], Step [135/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [136/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [137/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [138/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [139/156], Loss: 1.002\n",
      "Epoch [2/2000], Step [140/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [141/156], Loss: 0.996\n",
      "Epoch [2/2000], Step [142/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [143/156], Loss: 0.997\n",
      "Epoch [2/2000], Step [144/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [145/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [146/156], Loss: 0.998\n",
      "Epoch [2/2000], Step [147/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [148/156], Loss: 1.000\n",
      "Epoch [2/2000], Step [149/156], Loss: 1.003\n",
      "Epoch [2/2000], Step [150/156], Loss: 0.997\n",
      "Epoch [2/2000], Step [151/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [152/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [153/156], Loss: 0.997\n",
      "Epoch [2/2000], Step [154/156], Loss: 0.999\n",
      "Epoch [2/2000], Step [155/156], Loss: 1.001\n",
      "Epoch [2/2000], Step [156/156], Loss: 1.004\n",
      "Epoch [2/2000], Accumulated_Loss: 1.000\n",
      "Epoch [3/2000], Step [1/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [2/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [3/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [4/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [5/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [6/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [7/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [8/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [9/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [10/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [11/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [12/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [13/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [14/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [15/156], Loss: 0.997\n",
      "Epoch [3/2000], Step [16/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [17/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [18/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [19/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [20/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [21/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [22/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [23/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [24/156], Loss: 0.997\n",
      "Epoch [3/2000], Step [25/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [26/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [27/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [28/156], Loss: 1.004\n",
      "Epoch [3/2000], Step [29/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [30/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [31/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [32/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [33/156], Loss: 0.997\n",
      "Epoch [3/2000], Step [34/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [35/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [36/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [37/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [38/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [39/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [40/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [41/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [42/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [43/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [44/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [45/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [46/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [47/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [48/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [49/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [50/156], Loss: 0.997\n",
      "Epoch [3/2000], Step [51/156], Loss: 0.997\n",
      "Epoch [3/2000], Step [52/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [53/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [54/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [55/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [56/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [57/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [58/156], Loss: 1.003\n",
      "Epoch [3/2000], Step [59/156], Loss: 1.003\n",
      "Epoch [3/2000], Step [60/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [61/156], Loss: 1.003\n",
      "Epoch [3/2000], Step [62/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [63/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [64/156], Loss: 1.003\n",
      "Epoch [3/2000], Step [65/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [66/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [67/156], Loss: 0.997\n",
      "Epoch [3/2000], Step [68/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [69/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [70/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [71/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [72/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [73/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [74/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [75/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [76/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [77/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [78/156], Loss: 1.003\n",
      "Epoch [3/2000], Step [79/156], Loss: 0.995\n",
      "Epoch [3/2000], Step [80/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [81/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [82/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [83/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [84/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [85/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [86/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [87/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [88/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [89/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [90/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [91/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [92/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [93/156], Loss: 1.004\n",
      "Epoch [3/2000], Step [94/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [95/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [96/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [97/156], Loss: 1.004\n",
      "Epoch [3/2000], Step [98/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [99/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [100/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [101/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [102/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [103/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [104/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [105/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [106/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [107/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [108/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [109/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [110/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [111/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [112/156], Loss: 0.997\n",
      "Epoch [3/2000], Step [113/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [114/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [115/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [116/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [117/156], Loss: 1.004\n",
      "Epoch [3/2000], Step [118/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [119/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [120/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [121/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [122/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [123/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [124/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [125/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [126/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [127/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [128/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [129/156], Loss: 1.003\n",
      "Epoch [3/2000], Step [130/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [131/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [132/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [133/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [134/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [135/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [136/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [137/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [138/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [139/156], Loss: 0.997\n",
      "Epoch [3/2000], Step [140/156], Loss: 0.996\n",
      "Epoch [3/2000], Step [141/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [142/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [143/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [144/156], Loss: 1.001\n",
      "Epoch [3/2000], Step [145/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [146/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [147/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [148/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [149/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [150/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [151/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [152/156], Loss: 0.998\n",
      "Epoch [3/2000], Step [153/156], Loss: 1.002\n",
      "Epoch [3/2000], Step [154/156], Loss: 1.000\n",
      "Epoch [3/2000], Step [155/156], Loss: 0.999\n",
      "Epoch [3/2000], Step [156/156], Loss: 1.000\n",
      "Epoch [3/2000], Accumulated_Loss: 1.000\n",
      "Epoch [4/2000], Step [1/156], Loss: 1.003\n",
      "Epoch [4/2000], Step [2/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [3/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [4/156], Loss: 1.003\n",
      "Epoch [4/2000], Step [5/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [6/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [7/156], Loss: 0.997\n",
      "Epoch [4/2000], Step [8/156], Loss: 0.997\n",
      "Epoch [4/2000], Step [9/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [10/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [11/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [12/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [13/156], Loss: 1.004\n",
      "Epoch [4/2000], Step [14/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [15/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [16/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [17/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [18/156], Loss: 1.002\n",
      "Epoch [4/2000], Step [19/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [20/156], Loss: 0.997\n",
      "Epoch [4/2000], Step [21/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [22/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [23/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [24/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [25/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [26/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [27/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [28/156], Loss: 0.997\n",
      "Epoch [4/2000], Step [29/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [30/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [31/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [32/156], Loss: 0.997\n",
      "Epoch [4/2000], Step [33/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [34/156], Loss: 1.002\n",
      "Epoch [4/2000], Step [35/156], Loss: 1.005\n",
      "Epoch [4/2000], Step [36/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [37/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [38/156], Loss: 0.997\n",
      "Epoch [4/2000], Step [39/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [40/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [41/156], Loss: 0.997\n",
      "Epoch [4/2000], Step [42/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [43/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [44/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [45/156], Loss: 1.003\n",
      "Epoch [4/2000], Step [46/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [47/156], Loss: 0.997\n",
      "Epoch [4/2000], Step [48/156], Loss: 1.002\n",
      "Epoch [4/2000], Step [49/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [50/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [51/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [52/156], Loss: 1.002\n",
      "Epoch [4/2000], Step [53/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [54/156], Loss: 1.002\n",
      "Epoch [4/2000], Step [55/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [56/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [57/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [58/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [59/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [60/156], Loss: 1.002\n",
      "Epoch [4/2000], Step [61/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [62/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [63/156], Loss: 1.003\n",
      "Epoch [4/2000], Step [64/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [65/156], Loss: 1.003\n",
      "Epoch [4/2000], Step [66/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [67/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [68/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [69/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [70/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [71/156], Loss: 1.002\n",
      "Epoch [4/2000], Step [72/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [73/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [74/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [75/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [76/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [77/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [78/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [79/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [80/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [81/156], Loss: 0.997\n",
      "Epoch [4/2000], Step [82/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [83/156], Loss: 0.997\n",
      "Epoch [4/2000], Step [84/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [85/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [86/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [87/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [88/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [89/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [90/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [91/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [92/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [93/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [94/156], Loss: 1.002\n",
      "Epoch [4/2000], Step [95/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [96/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [97/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [98/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [99/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [100/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [101/156], Loss: 0.997\n",
      "Epoch [4/2000], Step [102/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [103/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [104/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [105/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [106/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [107/156], Loss: 1.002\n",
      "Epoch [4/2000], Step [108/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [109/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [110/156], Loss: 1.003\n",
      "Epoch [4/2000], Step [111/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [112/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [113/156], Loss: 0.996\n",
      "Epoch [4/2000], Step [114/156], Loss: 0.997\n",
      "Epoch [4/2000], Step [115/156], Loss: 0.997\n",
      "Epoch [4/2000], Step [116/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [117/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [118/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [119/156], Loss: 1.002\n",
      "Epoch [4/2000], Step [120/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [121/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [122/156], Loss: 1.002\n",
      "Epoch [4/2000], Step [123/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [124/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [125/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [126/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [127/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [128/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [129/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [130/156], Loss: 0.999\n",
      "Epoch [4/2000], Step [131/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [132/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [133/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [134/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [135/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [136/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [137/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [138/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [139/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [140/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [141/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [142/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [143/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [144/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [145/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [146/156], Loss: 1.002\n",
      "Epoch [4/2000], Step [147/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [148/156], Loss: 0.998\n",
      "Epoch [4/2000], Step [149/156], Loss: 1.003\n",
      "Epoch [4/2000], Step [150/156], Loss: 0.997\n",
      "Epoch [4/2000], Step [151/156], Loss: 0.997\n",
      "Epoch [4/2000], Step [152/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [153/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [154/156], Loss: 1.000\n",
      "Epoch [4/2000], Step [155/156], Loss: 1.001\n",
      "Epoch [4/2000], Step [156/156], Loss: 1.001\n",
      "Epoch [4/2000], Accumulated_Loss: 1.000\n",
      "Epoch [5/2000], Step [1/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [2/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [3/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [4/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [5/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [6/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [7/156], Loss: 0.997\n",
      "Epoch [5/2000], Step [8/156], Loss: 1.002\n",
      "Epoch [5/2000], Step [9/156], Loss: 1.003\n",
      "Epoch [5/2000], Step [10/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [11/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [12/156], Loss: 1.002\n",
      "Epoch [5/2000], Step [13/156], Loss: 1.004\n",
      "Epoch [5/2000], Step [14/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [15/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [16/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [17/156], Loss: 1.004\n",
      "Epoch [5/2000], Step [18/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [19/156], Loss: 0.997\n",
      "Epoch [5/2000], Step [20/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [21/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [22/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [23/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [24/156], Loss: 1.002\n",
      "Epoch [5/2000], Step [25/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [26/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [27/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [28/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [29/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [30/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [31/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [32/156], Loss: 1.002\n",
      "Epoch [5/2000], Step [33/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [34/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [35/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [36/156], Loss: 1.002\n",
      "Epoch [5/2000], Step [37/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [38/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [39/156], Loss: 0.994\n",
      "Epoch [5/2000], Step [40/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [41/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [42/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [43/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [44/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [45/156], Loss: 0.997\n",
      "Epoch [5/2000], Step [46/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [47/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [48/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [49/156], Loss: 1.002\n",
      "Epoch [5/2000], Step [50/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [51/156], Loss: 1.002\n",
      "Epoch [5/2000], Step [52/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [53/156], Loss: 1.002\n",
      "Epoch [5/2000], Step [54/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [55/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [56/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [57/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [58/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [59/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [60/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [61/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [62/156], Loss: 1.002\n",
      "Epoch [5/2000], Step [63/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [64/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [65/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [66/156], Loss: 1.002\n",
      "Epoch [5/2000], Step [67/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [68/156], Loss: 1.002\n",
      "Epoch [5/2000], Step [69/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [70/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [71/156], Loss: 1.002\n",
      "Epoch [5/2000], Step [72/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [73/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [74/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [75/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [76/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [77/156], Loss: 1.004\n",
      "Epoch [5/2000], Step [78/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [79/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [80/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [81/156], Loss: 1.002\n",
      "Epoch [5/2000], Step [82/156], Loss: 0.997\n",
      "Epoch [5/2000], Step [83/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [84/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [85/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [86/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [87/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [88/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [89/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [90/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [91/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [92/156], Loss: 1.003\n",
      "Epoch [5/2000], Step [93/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [94/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [95/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [96/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [97/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [98/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [99/156], Loss: 1.002\n",
      "Epoch [5/2000], Step [100/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [101/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [102/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [103/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [104/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [105/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [106/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [107/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [108/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [109/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [110/156], Loss: 0.997\n",
      "Epoch [5/2000], Step [111/156], Loss: 1.002\n",
      "Epoch [5/2000], Step [112/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [113/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [114/156], Loss: 0.996\n",
      "Epoch [5/2000], Step [115/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [116/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [117/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [118/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [119/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [120/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [121/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [122/156], Loss: 1.002\n",
      "Epoch [5/2000], Step [123/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [124/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [125/156], Loss: 0.997\n",
      "Epoch [5/2000], Step [126/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [127/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [128/156], Loss: 1.004\n",
      "Epoch [5/2000], Step [129/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [130/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [131/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [132/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [133/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [134/156], Loss: 1.002\n",
      "Epoch [5/2000], Step [135/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [136/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [137/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [138/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [139/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [140/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [141/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [142/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [143/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [144/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [145/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [146/156], Loss: 1.000\n",
      "Epoch [5/2000], Step [147/156], Loss: 1.002\n",
      "Epoch [5/2000], Step [148/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [149/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [150/156], Loss: 1.004\n",
      "Epoch [5/2000], Step [151/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [152/156], Loss: 1.003\n",
      "Epoch [5/2000], Step [153/156], Loss: 0.998\n",
      "Epoch [5/2000], Step [154/156], Loss: 1.001\n",
      "Epoch [5/2000], Step [155/156], Loss: 0.999\n",
      "Epoch [5/2000], Step [156/156], Loss: 1.002\n",
      "Epoch [5/2000], Accumulated_Loss: 1.000\n",
      "Epoch [6/2000], Step [1/156], Loss: 0.997\n",
      "Epoch [6/2000], Step [2/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [3/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [4/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [5/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [6/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [7/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [8/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [9/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [10/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [11/156], Loss: 1.004\n",
      "Epoch [6/2000], Step [12/156], Loss: 0.997\n",
      "Epoch [6/2000], Step [13/156], Loss: 0.997\n",
      "Epoch [6/2000], Step [14/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [15/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [16/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [17/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [18/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [19/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [20/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [21/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [22/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [23/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [24/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [25/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [26/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [27/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [28/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [29/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [30/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [31/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [32/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [33/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [34/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [35/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [36/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [37/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [38/156], Loss: 0.997\n",
      "Epoch [6/2000], Step [39/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [40/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [41/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [42/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [43/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [44/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [45/156], Loss: 1.003\n",
      "Epoch [6/2000], Step [46/156], Loss: 0.997\n",
      "Epoch [6/2000], Step [47/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [48/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [49/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [50/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [51/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [52/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [53/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [54/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [55/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [56/156], Loss: 0.996\n",
      "Epoch [6/2000], Step [57/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [58/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [59/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [60/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [61/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [62/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [63/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [64/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [65/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [66/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [67/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [68/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [69/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [70/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [71/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [72/156], Loss: 1.003\n",
      "Epoch [6/2000], Step [73/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [74/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [75/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [76/156], Loss: 0.997\n",
      "Epoch [6/2000], Step [77/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [78/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [79/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [80/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [81/156], Loss: 0.997\n",
      "Epoch [6/2000], Step [82/156], Loss: 0.997\n",
      "Epoch [6/2000], Step [83/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [84/156], Loss: 0.997\n",
      "Epoch [6/2000], Step [85/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [86/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [87/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [88/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [89/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [90/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [91/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [92/156], Loss: 1.004\n",
      "Epoch [6/2000], Step [93/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [94/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [95/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [96/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [97/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [98/156], Loss: 0.997\n",
      "Epoch [6/2000], Step [99/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [100/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [101/156], Loss: 0.997\n",
      "Epoch [6/2000], Step [102/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [103/156], Loss: 0.997\n",
      "Epoch [6/2000], Step [104/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [105/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [106/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [107/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [108/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [109/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [110/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [111/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [112/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [113/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [114/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [115/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [116/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [117/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [118/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [119/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [120/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [121/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [122/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [123/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [124/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [125/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [126/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [127/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [128/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [129/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [130/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [131/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [132/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [133/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [134/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [135/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [136/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [137/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [138/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [139/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [140/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [141/156], Loss: 1.003\n",
      "Epoch [6/2000], Step [142/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [143/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [144/156], Loss: 0.998\n",
      "Epoch [6/2000], Step [145/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [146/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [147/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [148/156], Loss: 1.002\n",
      "Epoch [6/2000], Step [149/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [150/156], Loss: 1.004\n",
      "Epoch [6/2000], Step [151/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [152/156], Loss: 0.999\n",
      "Epoch [6/2000], Step [153/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [154/156], Loss: 1.001\n",
      "Epoch [6/2000], Step [155/156], Loss: 1.000\n",
      "Epoch [6/2000], Step [156/156], Loss: 1.001\n",
      "Epoch [6/2000], Accumulated_Loss: 1.000\n",
      "Epoch [7/2000], Step [1/156], Loss: 1.003\n",
      "Epoch [7/2000], Step [2/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [3/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [4/156], Loss: 1.004\n",
      "Epoch [7/2000], Step [5/156], Loss: 1.002\n",
      "Epoch [7/2000], Step [6/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [7/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [8/156], Loss: 0.997\n",
      "Epoch [7/2000], Step [9/156], Loss: 0.997\n",
      "Epoch [7/2000], Step [10/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [11/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [12/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [13/156], Loss: 1.002\n",
      "Epoch [7/2000], Step [14/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [15/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [16/156], Loss: 0.995\n",
      "Epoch [7/2000], Step [17/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [18/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [19/156], Loss: 0.996\n",
      "Epoch [7/2000], Step [20/156], Loss: 1.002\n",
      "Epoch [7/2000], Step [21/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [22/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [23/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [24/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [25/156], Loss: 1.002\n",
      "Epoch [7/2000], Step [26/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [27/156], Loss: 1.002\n",
      "Epoch [7/2000], Step [28/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [29/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [30/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [31/156], Loss: 1.003\n",
      "Epoch [7/2000], Step [32/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [33/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [34/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [35/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [36/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [37/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [38/156], Loss: 1.002\n",
      "Epoch [7/2000], Step [39/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [40/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [41/156], Loss: 1.002\n",
      "Epoch [7/2000], Step [42/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [43/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [44/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [45/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [46/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [47/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [48/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [49/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [50/156], Loss: 0.997\n",
      "Epoch [7/2000], Step [51/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [52/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [53/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [54/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [55/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [56/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [57/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [58/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [59/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [60/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [61/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [62/156], Loss: 1.004\n",
      "Epoch [7/2000], Step [63/156], Loss: 1.002\n",
      "Epoch [7/2000], Step [64/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [65/156], Loss: 0.997\n",
      "Epoch [7/2000], Step [66/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [67/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [68/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [69/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [70/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [71/156], Loss: 1.002\n",
      "Epoch [7/2000], Step [72/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [73/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [74/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [75/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [76/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [77/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [78/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [79/156], Loss: 1.002\n",
      "Epoch [7/2000], Step [80/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [81/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [82/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [83/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [84/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [85/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [86/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [87/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [88/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [89/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [90/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [91/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [92/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [93/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [94/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [95/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [96/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [97/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [98/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [99/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [100/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [101/156], Loss: 1.002\n",
      "Epoch [7/2000], Step [102/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [103/156], Loss: 1.002\n",
      "Epoch [7/2000], Step [104/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [105/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [106/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [107/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [108/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [109/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [110/156], Loss: 1.002\n",
      "Epoch [7/2000], Step [111/156], Loss: 1.003\n",
      "Epoch [7/2000], Step [112/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [113/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [114/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [115/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [116/156], Loss: 1.002\n",
      "Epoch [7/2000], Step [117/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [118/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [119/156], Loss: 1.003\n",
      "Epoch [7/2000], Step [120/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [121/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [122/156], Loss: 0.996\n",
      "Epoch [7/2000], Step [123/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [124/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [125/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [126/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [127/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [128/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [129/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [130/156], Loss: 1.003\n",
      "Epoch [7/2000], Step [131/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [132/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [133/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [134/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [135/156], Loss: 1.002\n",
      "Epoch [7/2000], Step [136/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [137/156], Loss: 0.997\n",
      "Epoch [7/2000], Step [138/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [139/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [140/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [141/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [142/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [143/156], Loss: 0.997\n",
      "Epoch [7/2000], Step [144/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [145/156], Loss: 1.004\n",
      "Epoch [7/2000], Step [146/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [147/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [148/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [149/156], Loss: 1.002\n",
      "Epoch [7/2000], Step [150/156], Loss: 0.998\n",
      "Epoch [7/2000], Step [151/156], Loss: 1.002\n",
      "Epoch [7/2000], Step [152/156], Loss: 1.000\n",
      "Epoch [7/2000], Step [153/156], Loss: 0.999\n",
      "Epoch [7/2000], Step [154/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [155/156], Loss: 1.001\n",
      "Epoch [7/2000], Step [156/156], Loss: 1.000\n",
      "Epoch [7/2000], Accumulated_Loss: 1.000\n",
      "Epoch [8/2000], Step [1/156], Loss: 1.000\n",
      "Epoch [8/2000], Step [2/156], Loss: 0.998\n"
     ]
    }
   ],
   "source": [
    "# Training algorithm\n",
    "# x_0 ~ q(x_0)\n",
    "# t ~ Uniform({1, ...., T})\n",
    "# eps ~ N(0, I)\n",
    "# Take graident descent step on MSE(eps - eps_theta(root(alpha_t_bar)*x_0 + root(1-alpha_t_bar)*eps, t))\n",
    "# Until converged\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # to keep track of accumulated accuracy\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for step, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        t = torch.randint(1, T+1, (BATCH_SIZE,)).to(device)\n",
    "        epsilon = torch.randn_like(images)\n",
    "\n",
    "        noised_samples = (sqrt_alphas_cumprod[t - 1][(...,) + (None,) * 3] * images) + (sqrt_one_minus_alphas_cumprod[t - 1][(...,) + (None,) * 3] * epsilon)\n",
    "        # [(...,) + (None,) * 3] is used so that I can turn the tensor of shape N to N 1 1 1, which allows me to multiply it with images and epsilon which have shape N C W H\n",
    "\n",
    "        # Forward pass\n",
    "        predicted_epsilon = model(noised_samples, t)\n",
    "        loss = criterion(predicted_epsilon, epsilon)\n",
    "        epoch_loss += loss\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step+1) % 1 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{EPOCHS}], Step [{step+1}/{len(dataloader)}], Loss: {loss.item():.3f}')\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Accumulated_Loss: {(epoch_loss.item() / (len(dataloader))):.3f}')\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_diffusion_sample(x_0, t, device=\"cpu\"):\n",
    "    noise = torch.randn(1, 3, 64, 64).to(device)\n",
    "    xt = (sqrt_alphas_cumprod[t - 1][(...,) + (None,) * 3] * x_0) + (sqrt_one_minus_alphas_cumprod[t - 1][(...,) + (None,) * 3] * noise)\n",
    "\n",
    "    return xt, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 299 is out of bounds for dimension 0 with size 250",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[1;32m      5\u001b[0m x_current \u001b[38;5;241m=\u001b[39m images[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 6\u001b[0m x_current, noise \u001b[38;5;241m=\u001b[39m \u001b[43mforward_diffusion_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_current\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m show_tensor_image(x_current\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      9\u001b[0m show_tensor_image(x_current\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m, in \u001b[0;36mforward_diffusion_sample\u001b[0;34m(x_0, t, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_diffusion_sample\u001b[39m(x_0, t, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      2\u001b[0m     noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m     xt \u001b[38;5;241m=\u001b[39m (\u001b[43msqrt_alphas_cumprod\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[(\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m*\u001b[39m x_0) \u001b[38;5;241m+\u001b[39m (sqrt_one_minus_alphas_cumprod[t \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m][(\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28;01mNone\u001b[39;00m,) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m*\u001b[39m noise)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xt, noise\n",
      "\u001b[0;31mIndexError\u001b[0m: index 299 is out of bounds for dimension 0 with size 250"
     ]
    }
   ],
   "source": [
    "images, labels = next(dataiter)\n",
    "\n",
    "t = 150\n",
    "\n",
    "x_current = images[0].unsqueeze(0).to(device)\n",
    "x_current, noise = forward_diffusion_sample(x_current, t, device)\n",
    "\n",
    "show_tensor_image(x_current.to('cpu'))\n",
    "show_tensor_image(x_current.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'sapl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Sampling algorithm\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# for t = T, ... 1 do\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# z ~ N(0, I) if t > 1, else z =0\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# x_t-1 = 1/root(alpha_t) * (x_t - (1-alpha_t)/(root(1-alpha_t))*eps_theta(x_t, t)) + sigma_t*z\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# end for\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# return x_0\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m x_current \u001b[38;5;241m=\u001b[39m \u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msapl\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     11\u001b[0m     x_current \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'sapl'"
     ]
    }
   ],
   "source": [
    "# Sampling algorithm\n",
    "# for t = T, ... 1 do\n",
    "# z ~ N(0, I) if t > 1, else z =0\n",
    "# x_t-1 = 1/root(alpha_t) * (x_t - (1-alpha_t)/(root(1-alpha_t))*eps_theta(x_t, t)) + sigma_t*z\n",
    "# end for\n",
    "# return x_0\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_current = torch.randn(1, 3, 64, 64).to(device)\n",
    "    for i in range(T, 0, -1):\n",
    "        z = torch.randn(1, 3, 64, 64).to(device) if i > 0 else torch.zeros(1, 3, 64, 64).to(device)\n",
    "        epsilon_theta = model(x_current, torch.tensor([i - 1], device=device))\n",
    "        x_prev = sqrt_recip_alphas[i - 1] * (x_current - (((1 - alphas[i - 1])/(sqrt_one_minus_alphas_cumprod[i-1]))*epsilon_theta))\n",
    "\n",
    "    show_tensor_image(x_prev.cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-3.9.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
