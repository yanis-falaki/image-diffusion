{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import UNetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "\n",
    "def quadratic_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    # Generate a linear space from 0 to 1\n",
    "    linear_space = torch.linspace(0, 1, timesteps)\n",
    "    \n",
    "    # Apply a quadratic transformation\n",
    "    quadratic_space = linear_space ** 2\n",
    "    \n",
    "    # Scale and shift the quadratic space to start and end at the specified values\n",
    "    beta_values = start + (end - start) * quadratic_space\n",
    "    \n",
    "    return beta_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 2000\n",
    "\n",
    "# Define beta schedule\n",
    "T = 250\n",
    "betas = linear_beta_schedule(timesteps=T).to(device)\n",
    "\n",
    "# Pre-calculate different terms for closed form\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0/ alphas)\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transformed_dataset():\n",
    "    data_transforms = [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(), # Scales data into [0, 1]\n",
    "        transforms.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1]\n",
    "    ]\n",
    "\n",
    "    data_transform = transforms.Compose(data_transforms)\n",
    "\n",
    "    train = torchvision.datasets.Flowers102(root=\".\", download=True, transform=data_transform)\n",
    "\n",
    "    test = torchvision.datasets.Flowers102(root=\".\", download=True, transform=data_transform, split='test')\n",
    "\n",
    "    return torch.utils.data.ConcatDataset([train, test])\n",
    "\n",
    "def show_tensor_image(image):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "        transforms.Lambda(lambda t: t * 255.),\n",
    "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "\n",
    "    # take first image of batch\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0, :, :, :]\n",
    "    plt.imshow(reverse_transforms(image))\n",
    "\n",
    "data = load_transformed_dataset()\n",
    "dataloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, prefetch_factor=4, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetv2.GoodUNet(channel_sequence=[48, 96, 192, 384], time_emb_dim = 48, n=1).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(weight_decay=1e-4, params=model.parameters(), lr=0.05)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=5, gamma=0.5)\n",
    "checkpoint_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5064819"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Load checkpoint\\ncheckpoint = torch.load('./checkpoints/model_checkpoint_10.pth')\\n\\nmodel.load_state_dict(checkpoint['model_state_dict'])\\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\\nscheduler.load_state_dict(checkpoint['scheduler_state_dict'])\\ncheckpoint_epoch = checkpoint.get('epoch', None)\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Load checkpoint\n",
    "checkpoint = torch.load('./checkpoints/model_checkpoint_10.pth')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "checkpoint_epoch = checkpoint.get('epoch', None)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000], Step [1/56], Loss: 1.189\n",
      "Epoch [1/2000], Step [2/56], Loss: 3.581\n",
      "Epoch [1/2000], Step [3/56], Loss: 1.595\n",
      "Epoch [1/2000], Step [4/56], Loss: 1.107\n",
      "Epoch [1/2000], Step [5/56], Loss: 1.158\n",
      "Epoch [1/2000], Step [6/56], Loss: 1.250\n",
      "Epoch [1/2000], Step [7/56], Loss: 1.146\n",
      "Epoch [1/2000], Step [8/56], Loss: 1.090\n",
      "Epoch [1/2000], Step [9/56], Loss: 1.119\n",
      "Epoch [1/2000], Step [10/56], Loss: 1.071\n",
      "Epoch [1/2000], Step [11/56], Loss: 1.017\n",
      "Epoch [1/2000], Step [12/56], Loss: 1.021\n",
      "Epoch [1/2000], Step [13/56], Loss: 1.051\n",
      "Epoch [1/2000], Step [14/56], Loss: 1.063\n",
      "Epoch [1/2000], Step [15/56], Loss: 1.048\n",
      "Epoch [1/2000], Step [16/56], Loss: 1.018\n",
      "Epoch [1/2000], Step [17/56], Loss: 1.001\n",
      "Epoch [1/2000], Step [18/56], Loss: 1.009\n",
      "Epoch [1/2000], Step [19/56], Loss: 1.021\n",
      "Epoch [1/2000], Step [20/56], Loss: 1.023\n",
      "Epoch [1/2000], Step [21/56], Loss: 1.021\n",
      "Epoch [1/2000], Step [22/56], Loss: 1.018\n",
      "Epoch [1/2000], Step [23/56], Loss: 1.011\n",
      "Epoch [1/2000], Step [24/56], Loss: 1.005\n",
      "Epoch [1/2000], Step [25/56], Loss: 1.005\n",
      "Epoch [1/2000], Step [26/56], Loss: 1.003\n",
      "Epoch [1/2000], Step [27/56], Loss: 1.005\n",
      "Epoch [1/2000], Step [28/56], Loss: 1.009\n",
      "Epoch [1/2000], Step [29/56], Loss: 1.010\n",
      "Epoch [1/2000], Step [30/56], Loss: 1.010\n",
      "Epoch [1/2000], Step [31/56], Loss: 1.005\n",
      "Epoch [1/2000], Step [32/56], Loss: 0.997\n",
      "Epoch [1/2000], Step [33/56], Loss: 1.001\n",
      "Epoch [1/2000], Step [34/56], Loss: 1.006\n",
      "Epoch [1/2000], Step [35/56], Loss: 1.004\n",
      "Epoch [1/2000], Step [36/56], Loss: 1.000\n",
      "Epoch [1/2000], Step [37/56], Loss: 0.999\n",
      "Epoch [1/2000], Step [38/56], Loss: 1.001\n",
      "Epoch [1/2000], Step [39/56], Loss: 1.000\n",
      "Epoch [1/2000], Step [40/56], Loss: 1.003\n",
      "Epoch [1/2000], Step [41/56], Loss: 1.001\n",
      "Epoch [1/2000], Step [42/56], Loss: 1.002\n",
      "Epoch [1/2000], Step [43/56], Loss: 1.002\n",
      "Epoch [1/2000], Step [44/56], Loss: 1.002\n",
      "Epoch [1/2000], Step [45/56], Loss: 0.999\n",
      "Epoch [1/2000], Step [46/56], Loss: 0.999\n",
      "Epoch [1/2000], Step [47/56], Loss: 1.000\n",
      "Epoch [1/2000], Step [48/56], Loss: 1.000\n",
      "Epoch [1/2000], Step [49/56], Loss: 1.002\n",
      "Epoch [1/2000], Step [50/56], Loss: 1.000\n",
      "Epoch [1/2000], Step [51/56], Loss: 1.001\n",
      "Epoch [1/2000], Step [52/56], Loss: 1.000\n",
      "Epoch [1/2000], Step [53/56], Loss: 1.001\n",
      "Epoch [1/2000], Step [54/56], Loss: 1.000\n",
      "Epoch [1/2000], Step [55/56], Loss: 0.998\n",
      "Epoch [1/2000], Step [56/56], Loss: 1.001\n",
      "Epoch [1/2000], Accumulated_Loss: 1.084\n",
      "Epoch [2/2000], Step [1/56], Loss: 1.000\n",
      "Epoch [2/2000], Step [2/56], Loss: 1.000\n",
      "Epoch [2/2000], Step [3/56], Loss: 0.999\n",
      "Epoch [2/2000], Step [4/56], Loss: 1.000\n",
      "Epoch [2/2000], Step [5/56], Loss: 1.000\n",
      "Epoch [2/2000], Step [6/56], Loss: 0.999\n",
      "Epoch [2/2000], Step [7/56], Loss: 0.998\n",
      "Epoch [2/2000], Step [8/56], Loss: 0.998\n",
      "Epoch [2/2000], Step [9/56], Loss: 1.000\n",
      "Epoch [2/2000], Step [10/56], Loss: 0.997\n",
      "Epoch [2/2000], Step [11/56], Loss: 0.998\n",
      "Epoch [2/2000], Step [12/56], Loss: 1.001\n",
      "Epoch [2/2000], Step [13/56], Loss: 0.998\n",
      "Epoch [2/2000], Step [14/56], Loss: 0.999\n",
      "Epoch [2/2000], Step [15/56], Loss: 0.998\n",
      "Epoch [2/2000], Step [16/56], Loss: 0.998\n",
      "Epoch [2/2000], Step [17/56], Loss: 1.000\n",
      "Epoch [2/2000], Step [18/56], Loss: 0.999\n",
      "Epoch [2/2000], Step [19/56], Loss: 0.996\n",
      "Epoch [2/2000], Step [20/56], Loss: 0.998\n",
      "Epoch [2/2000], Step [21/56], Loss: 0.998\n",
      "Epoch [2/2000], Step [22/56], Loss: 0.997\n",
      "Epoch [2/2000], Step [23/56], Loss: 0.998\n",
      "Epoch [2/2000], Step [24/56], Loss: 0.999\n",
      "Epoch [2/2000], Step [25/56], Loss: 0.995\n",
      "Epoch [2/2000], Step [26/56], Loss: 0.999\n",
      "Epoch [2/2000], Step [27/56], Loss: 1.001\n",
      "Epoch [2/2000], Step [28/56], Loss: 0.998\n",
      "Epoch [2/2000], Step [29/56], Loss: 0.995\n",
      "Epoch [2/2000], Step [30/56], Loss: 1.000\n",
      "Epoch [2/2000], Step [31/56], Loss: 0.996\n",
      "Epoch [2/2000], Step [32/56], Loss: 0.997\n",
      "Epoch [2/2000], Step [33/56], Loss: 0.995\n",
      "Epoch [2/2000], Step [34/56], Loss: 0.996\n",
      "Epoch [2/2000], Step [35/56], Loss: 0.995\n",
      "Epoch [2/2000], Step [36/56], Loss: 0.996\n",
      "Epoch [2/2000], Step [37/56], Loss: 0.996\n",
      "Epoch [2/2000], Step [38/56], Loss: 0.997\n",
      "Epoch [2/2000], Step [39/56], Loss: 0.996\n",
      "Epoch [2/2000], Step [40/56], Loss: 0.992\n",
      "Epoch [2/2000], Step [41/56], Loss: 0.995\n",
      "Epoch [2/2000], Step [42/56], Loss: 0.992\n",
      "Epoch [2/2000], Step [43/56], Loss: 0.992\n",
      "Epoch [2/2000], Step [44/56], Loss: 0.990\n",
      "Epoch [2/2000], Step [45/56], Loss: 0.989\n",
      "Epoch [2/2000], Step [46/56], Loss: 0.991\n",
      "Epoch [2/2000], Step [47/56], Loss: 0.990\n",
      "Epoch [2/2000], Step [48/56], Loss: 0.987\n",
      "Epoch [2/2000], Step [49/56], Loss: 0.987\n",
      "Epoch [2/2000], Step [50/56], Loss: 0.984\n",
      "Epoch [2/2000], Step [51/56], Loss: 0.986\n",
      "Epoch [2/2000], Step [52/56], Loss: 0.984\n",
      "Epoch [2/2000], Step [53/56], Loss: 0.983\n",
      "Epoch [2/2000], Step [54/56], Loss: 0.983\n",
      "Epoch [2/2000], Step [55/56], Loss: 0.981\n",
      "Epoch [2/2000], Step [56/56], Loss: 0.981\n",
      "Epoch [2/2000], Accumulated_Loss: 0.995\n",
      "Epoch [3/2000], Step [1/56], Loss: 0.981\n",
      "Epoch [3/2000], Step [2/56], Loss: 0.981\n",
      "Epoch [3/2000], Step [3/56], Loss: 0.975\n",
      "Epoch [3/2000], Step [4/56], Loss: 0.965\n",
      "Epoch [3/2000], Step [5/56], Loss: 0.963\n",
      "Epoch [3/2000], Step [6/56], Loss: 0.954\n",
      "Epoch [3/2000], Step [7/56], Loss: 0.941\n",
      "Epoch [3/2000], Step [8/56], Loss: 0.926\n",
      "Epoch [3/2000], Step [9/56], Loss: 0.900\n",
      "Epoch [3/2000], Step [10/56], Loss: 0.882\n",
      "Epoch [3/2000], Step [11/56], Loss: 0.855\n",
      "Epoch [3/2000], Step [12/56], Loss: 0.839\n",
      "Epoch [3/2000], Step [13/56], Loss: 0.840\n",
      "Epoch [3/2000], Step [14/56], Loss: 0.811\n",
      "Epoch [3/2000], Step [15/56], Loss: 0.805\n",
      "Epoch [3/2000], Step [16/56], Loss: 0.800\n",
      "Epoch [3/2000], Step [17/56], Loss: 0.789\n",
      "Epoch [3/2000], Step [18/56], Loss: 0.773\n",
      "Epoch [3/2000], Step [19/56], Loss: 0.769\n",
      "Epoch [3/2000], Step [20/56], Loss: 0.769\n",
      "Epoch [3/2000], Step [21/56], Loss: 0.754\n",
      "Epoch [3/2000], Step [22/56], Loss: 0.760\n",
      "Epoch [3/2000], Step [23/56], Loss: 0.756\n",
      "Epoch [3/2000], Step [24/56], Loss: 0.751\n",
      "Epoch [3/2000], Step [25/56], Loss: 0.751\n",
      "Epoch [3/2000], Step [26/56], Loss: 0.745\n",
      "Epoch [3/2000], Step [27/56], Loss: 0.733\n",
      "Epoch [3/2000], Step [28/56], Loss: 0.738\n",
      "Epoch [3/2000], Step [29/56], Loss: 0.743\n",
      "Epoch [3/2000], Step [30/56], Loss: 0.731\n",
      "Epoch [3/2000], Step [31/56], Loss: 0.742\n",
      "Epoch [3/2000], Step [32/56], Loss: 0.730\n",
      "Epoch [3/2000], Step [33/56], Loss: 0.728\n",
      "Epoch [3/2000], Step [34/56], Loss: 0.735\n",
      "Epoch [3/2000], Step [35/56], Loss: 0.747\n",
      "Epoch [3/2000], Step [36/56], Loss: 0.723\n",
      "Epoch [3/2000], Step [37/56], Loss: 0.723\n",
      "Epoch [3/2000], Step [38/56], Loss: 0.720\n",
      "Epoch [3/2000], Step [39/56], Loss: 0.724\n",
      "Epoch [3/2000], Step [40/56], Loss: 0.729\n",
      "Epoch [3/2000], Step [41/56], Loss: 0.720\n",
      "Epoch [3/2000], Step [42/56], Loss: 0.734\n",
      "Epoch [3/2000], Step [43/56], Loss: 0.716\n",
      "Epoch [3/2000], Step [44/56], Loss: 0.719\n",
      "Epoch [3/2000], Step [45/56], Loss: 0.724\n",
      "Epoch [3/2000], Step [46/56], Loss: 0.725\n",
      "Epoch [3/2000], Step [47/56], Loss: 0.716\n",
      "Epoch [3/2000], Step [48/56], Loss: 0.712\n",
      "Epoch [3/2000], Step [49/56], Loss: 0.717\n",
      "Epoch [3/2000], Step [50/56], Loss: 0.700\n",
      "Epoch [3/2000], Step [51/56], Loss: 0.713\n",
      "Epoch [3/2000], Step [52/56], Loss: 0.712\n",
      "Epoch [3/2000], Step [53/56], Loss: 0.691\n",
      "Epoch [3/2000], Step [54/56], Loss: 0.690\n",
      "Epoch [3/2000], Step [55/56], Loss: 0.670\n",
      "Epoch [3/2000], Step [56/56], Loss: 0.638\n",
      "Epoch [3/2000], Accumulated_Loss: 0.778\n",
      "Epoch [4/2000], Step [1/56], Loss: 0.600\n",
      "Epoch [4/2000], Step [2/56], Loss: 0.576\n",
      "Epoch [4/2000], Step [3/56], Loss: 0.551\n",
      "Epoch [4/2000], Step [4/56], Loss: 0.587\n",
      "Epoch [4/2000], Step [5/56], Loss: 0.575\n",
      "Epoch [4/2000], Step [6/56], Loss: 0.506\n",
      "Epoch [4/2000], Step [7/56], Loss: 0.541\n",
      "Epoch [4/2000], Step [8/56], Loss: 0.478\n",
      "Epoch [4/2000], Step [9/56], Loss: 0.495\n",
      "Epoch [4/2000], Step [10/56], Loss: 0.467\n",
      "Epoch [4/2000], Step [11/56], Loss: 0.465\n",
      "Epoch [4/2000], Step [12/56], Loss: 0.457\n",
      "Epoch [4/2000], Step [13/56], Loss: 0.457\n",
      "Epoch [4/2000], Step [14/56], Loss: 0.467\n",
      "Epoch [4/2000], Step [15/56], Loss: 0.439\n",
      "Epoch [4/2000], Step [16/56], Loss: 0.452\n",
      "Epoch [4/2000], Step [17/56], Loss: 0.448\n",
      "Epoch [4/2000], Step [18/56], Loss: 0.444\n",
      "Epoch [4/2000], Step [19/56], Loss: 0.456\n",
      "Epoch [4/2000], Step [20/56], Loss: 0.426\n",
      "Epoch [4/2000], Step [21/56], Loss: 0.445\n",
      "Epoch [4/2000], Step [22/56], Loss: 0.430\n",
      "Epoch [4/2000], Step [23/56], Loss: 0.446\n",
      "Epoch [4/2000], Step [24/56], Loss: 0.441\n",
      "Epoch [4/2000], Step [25/56], Loss: 0.427\n",
      "Epoch [4/2000], Step [26/56], Loss: 0.426\n",
      "Epoch [4/2000], Step [27/56], Loss: 0.410\n",
      "Epoch [4/2000], Step [28/56], Loss: 0.442\n",
      "Epoch [4/2000], Step [29/56], Loss: 0.419\n",
      "Epoch [4/2000], Step [30/56], Loss: 0.433\n",
      "Epoch [4/2000], Step [31/56], Loss: 0.446\n",
      "Epoch [4/2000], Step [32/56], Loss: 0.426\n",
      "Epoch [4/2000], Step [33/56], Loss: 0.417\n",
      "Epoch [4/2000], Step [34/56], Loss: 0.388\n",
      "Epoch [4/2000], Step [35/56], Loss: 0.389\n",
      "Epoch [4/2000], Step [36/56], Loss: 0.383\n",
      "Epoch [4/2000], Step [37/56], Loss: 0.386\n",
      "Epoch [4/2000], Step [38/56], Loss: 0.371\n",
      "Epoch [4/2000], Step [39/56], Loss: 0.354\n",
      "Epoch [4/2000], Step [40/56], Loss: 0.321\n",
      "Epoch [4/2000], Step [41/56], Loss: 0.320\n",
      "Epoch [4/2000], Step [42/56], Loss: 0.309\n",
      "Epoch [4/2000], Step [43/56], Loss: 0.268\n",
      "Epoch [4/2000], Step [44/56], Loss: 0.253\n",
      "Epoch [4/2000], Step [45/56], Loss: 0.224\n",
      "Epoch [4/2000], Step [46/56], Loss: 0.212\n",
      "Epoch [4/2000], Step [47/56], Loss: 0.172\n",
      "Epoch [4/2000], Step [48/56], Loss: 0.161\n",
      "Epoch [4/2000], Step [49/56], Loss: 0.189\n",
      "Epoch [4/2000], Step [50/56], Loss: 0.191\n",
      "Epoch [4/2000], Step [51/56], Loss: 0.230\n",
      "Epoch [4/2000], Step [52/56], Loss: 0.201\n",
      "Epoch [4/2000], Step [53/56], Loss: 0.161\n",
      "Epoch [4/2000], Step [54/56], Loss: 0.172\n",
      "Epoch [4/2000], Step [55/56], Loss: 0.174\n",
      "Epoch [4/2000], Step [56/56], Loss: 0.163\n",
      "Epoch [4/2000], Accumulated_Loss: 0.384\n",
      "Epoch [5/2000], Step [1/56], Loss: 0.150\n",
      "Epoch [5/2000], Step [2/56], Loss: 0.128\n",
      "Epoch [5/2000], Step [3/56], Loss: 0.162\n",
      "Epoch [5/2000], Step [4/56], Loss: 0.123\n",
      "Epoch [5/2000], Step [5/56], Loss: 0.149\n",
      "Epoch [5/2000], Step [6/56], Loss: 0.153\n",
      "Epoch [5/2000], Step [7/56], Loss: 0.133\n",
      "Epoch [5/2000], Step [8/56], Loss: 0.155\n",
      "Epoch [5/2000], Step [9/56], Loss: 0.132\n",
      "Epoch [5/2000], Step [10/56], Loss: 0.127\n",
      "Epoch [5/2000], Step [11/56], Loss: 0.113\n",
      "Epoch [5/2000], Step [12/56], Loss: 0.133\n",
      "Epoch [5/2000], Step [13/56], Loss: 0.119\n",
      "Epoch [5/2000], Step [14/56], Loss: 0.133\n",
      "Epoch [5/2000], Step [15/56], Loss: 0.112\n",
      "Epoch [5/2000], Step [16/56], Loss: 0.120\n",
      "Epoch [5/2000], Step [17/56], Loss: 0.139\n",
      "Epoch [5/2000], Step [18/56], Loss: 0.112\n",
      "Epoch [5/2000], Step [19/56], Loss: 0.093\n",
      "Epoch [5/2000], Step [20/56], Loss: 0.121\n",
      "Epoch [5/2000], Step [21/56], Loss: 0.136\n",
      "Epoch [5/2000], Step [22/56], Loss: 0.124\n",
      "Epoch [5/2000], Step [23/56], Loss: 0.126\n",
      "Epoch [5/2000], Step [24/56], Loss: 0.138\n",
      "Epoch [5/2000], Step [25/56], Loss: 0.130\n",
      "Epoch [5/2000], Step [26/56], Loss: 0.136\n",
      "Epoch [5/2000], Step [27/56], Loss: 0.124\n",
      "Epoch [5/2000], Step [28/56], Loss: 0.130\n",
      "Epoch [5/2000], Step [29/56], Loss: 0.117\n",
      "Epoch [5/2000], Step [30/56], Loss: 0.120\n",
      "Epoch [5/2000], Step [31/56], Loss: 0.099\n",
      "Epoch [5/2000], Step [32/56], Loss: 0.114\n",
      "Epoch [5/2000], Step [33/56], Loss: 0.128\n",
      "Epoch [5/2000], Step [34/56], Loss: 0.118\n",
      "Epoch [5/2000], Step [35/56], Loss: 0.141\n",
      "Epoch [5/2000], Step [36/56], Loss: 0.076\n",
      "Epoch [5/2000], Step [37/56], Loss: 0.106\n",
      "Epoch [5/2000], Step [38/56], Loss: 0.118\n",
      "Epoch [5/2000], Step [39/56], Loss: 0.122\n",
      "Epoch [5/2000], Step [40/56], Loss: 0.136\n",
      "Epoch [5/2000], Step [41/56], Loss: 0.111\n",
      "Epoch [5/2000], Step [42/56], Loss: 0.106\n",
      "Epoch [5/2000], Step [43/56], Loss: 0.143\n",
      "Epoch [5/2000], Step [44/56], Loss: 0.100\n",
      "Epoch [5/2000], Step [45/56], Loss: 0.105\n",
      "Epoch [5/2000], Step [46/56], Loss: 0.127\n",
      "Epoch [5/2000], Step [47/56], Loss: 0.110\n",
      "Epoch [5/2000], Step [48/56], Loss: 0.089\n",
      "Epoch [5/2000], Step [49/56], Loss: 0.132\n",
      "Epoch [5/2000], Step [50/56], Loss: 0.125\n",
      "Epoch [5/2000], Step [51/56], Loss: 0.125\n",
      "Epoch [5/2000], Step [52/56], Loss: 0.145\n",
      "Epoch [5/2000], Step [53/56], Loss: 0.112\n",
      "Epoch [5/2000], Step [54/56], Loss: 0.110\n",
      "Epoch [5/2000], Step [55/56], Loss: 0.107\n",
      "Epoch [5/2000], Step [56/56], Loss: 0.165\n",
      "Epoch [5/2000], Accumulated_Loss: 0.124\n",
      "Epoch [6/2000], Step [1/56], Loss: 0.118\n",
      "Epoch [6/2000], Step [2/56], Loss: 0.100\n",
      "Epoch [6/2000], Step [3/56], Loss: 0.120\n",
      "Epoch [6/2000], Step [4/56], Loss: 0.143\n",
      "Epoch [6/2000], Step [5/56], Loss: 0.115\n",
      "Epoch [6/2000], Step [6/56], Loss: 0.112\n",
      "Epoch [6/2000], Step [7/56], Loss: 0.126\n",
      "Epoch [6/2000], Step [8/56], Loss: 0.100\n",
      "Epoch [6/2000], Step [9/56], Loss: 0.117\n",
      "Epoch [6/2000], Step [10/56], Loss: 0.112\n",
      "Epoch [6/2000], Step [11/56], Loss: 0.145\n",
      "Epoch [6/2000], Step [12/56], Loss: 0.111\n",
      "Epoch [6/2000], Step [13/56], Loss: 0.098\n",
      "Epoch [6/2000], Step [14/56], Loss: 0.133\n",
      "Epoch [6/2000], Step [15/56], Loss: 0.102\n",
      "Epoch [6/2000], Step [16/56], Loss: 0.108\n",
      "Epoch [6/2000], Step [17/56], Loss: 0.110\n",
      "Epoch [6/2000], Step [18/56], Loss: 0.104\n",
      "Epoch [6/2000], Step [19/56], Loss: 0.101\n",
      "Epoch [6/2000], Step [20/56], Loss: 0.107\n",
      "Epoch [6/2000], Step [21/56], Loss: 0.105\n",
      "Epoch [6/2000], Step [22/56], Loss: 0.113\n",
      "Epoch [6/2000], Step [23/56], Loss: 0.092\n",
      "Epoch [6/2000], Step [24/56], Loss: 0.125\n",
      "Epoch [6/2000], Step [25/56], Loss: 0.100\n",
      "Epoch [6/2000], Step [26/56], Loss: 0.140\n",
      "Epoch [6/2000], Step [27/56], Loss: 0.129\n",
      "Epoch [6/2000], Step [28/56], Loss: 0.123\n",
      "Epoch [6/2000], Step [29/56], Loss: 0.097\n",
      "Epoch [6/2000], Step [30/56], Loss: 0.116\n",
      "Epoch [6/2000], Step [31/56], Loss: 0.096\n",
      "Epoch [6/2000], Step [32/56], Loss: 0.112\n",
      "Epoch [6/2000], Step [33/56], Loss: 0.114\n",
      "Epoch [6/2000], Step [34/56], Loss: 0.089\n",
      "Epoch [6/2000], Step [35/56], Loss: 0.133\n",
      "Epoch [6/2000], Step [36/56], Loss: 0.118\n",
      "Epoch [6/2000], Step [37/56], Loss: 0.080\n",
      "Epoch [6/2000], Step [38/56], Loss: 0.097\n",
      "Epoch [6/2000], Step [39/56], Loss: 0.102\n",
      "Epoch [6/2000], Step [40/56], Loss: 0.122\n",
      "Epoch [6/2000], Step [41/56], Loss: 0.110\n",
      "Epoch [6/2000], Step [42/56], Loss: 0.097\n",
      "Epoch [6/2000], Step [43/56], Loss: 0.105\n",
      "Epoch [6/2000], Step [44/56], Loss: 0.104\n",
      "Epoch [6/2000], Step [45/56], Loss: 0.112\n",
      "Epoch [6/2000], Step [46/56], Loss: 0.110\n",
      "Epoch [6/2000], Step [47/56], Loss: 0.102\n",
      "Epoch [6/2000], Step [48/56], Loss: 0.108\n",
      "Epoch [6/2000], Step [49/56], Loss: 0.082\n",
      "Epoch [6/2000], Step [50/56], Loss: 0.091\n",
      "Epoch [6/2000], Step [51/56], Loss: 0.114\n",
      "Epoch [6/2000], Step [52/56], Loss: 0.128\n",
      "Epoch [6/2000], Step [53/56], Loss: 0.106\n",
      "Epoch [6/2000], Step [54/56], Loss: 0.089\n",
      "Epoch [6/2000], Step [55/56], Loss: 0.112\n",
      "Epoch [6/2000], Step [56/56], Loss: 0.100\n",
      "Epoch [6/2000], Accumulated_Loss: 0.110\n",
      "Epoch [7/2000], Step [1/56], Loss: 0.114\n",
      "Epoch [7/2000], Step [2/56], Loss: 0.091\n",
      "Epoch [7/2000], Step [3/56], Loss: 0.106\n",
      "Epoch [7/2000], Step [4/56], Loss: 0.094\n",
      "Epoch [7/2000], Step [5/56], Loss: 0.086\n",
      "Epoch [7/2000], Step [6/56], Loss: 0.106\n",
      "Epoch [7/2000], Step [7/56], Loss: 0.087\n",
      "Epoch [7/2000], Step [8/56], Loss: 0.107\n",
      "Epoch [7/2000], Step [9/56], Loss: 0.117\n",
      "Epoch [7/2000], Step [10/56], Loss: 0.113\n",
      "Epoch [7/2000], Step [11/56], Loss: 0.112\n",
      "Epoch [7/2000], Step [12/56], Loss: 0.108\n",
      "Epoch [7/2000], Step [13/56], Loss: 0.124\n",
      "Epoch [7/2000], Step [14/56], Loss: 0.097\n",
      "Epoch [7/2000], Step [15/56], Loss: 0.113\n",
      "Epoch [7/2000], Step [16/56], Loss: 0.078\n",
      "Epoch [7/2000], Step [17/56], Loss: 0.116\n",
      "Epoch [7/2000], Step [18/56], Loss: 0.099\n",
      "Epoch [7/2000], Step [19/56], Loss: 0.129\n",
      "Epoch [7/2000], Step [20/56], Loss: 0.124\n",
      "Epoch [7/2000], Step [21/56], Loss: 0.109\n",
      "Epoch [7/2000], Step [22/56], Loss: 0.121\n",
      "Epoch [7/2000], Step [23/56], Loss: 0.120\n",
      "Epoch [7/2000], Step [24/56], Loss: 0.103\n",
      "Epoch [7/2000], Step [25/56], Loss: 0.101\n",
      "Epoch [7/2000], Step [26/56], Loss: 0.107\n",
      "Epoch [7/2000], Step [27/56], Loss: 0.113\n",
      "Epoch [7/2000], Step [28/56], Loss: 0.119\n",
      "Epoch [7/2000], Step [29/56], Loss: 0.086\n",
      "Epoch [7/2000], Step [30/56], Loss: 0.108\n",
      "Epoch [7/2000], Step [31/56], Loss: 0.120\n",
      "Epoch [7/2000], Step [32/56], Loss: 0.104\n",
      "Epoch [7/2000], Step [33/56], Loss: 0.088\n",
      "Epoch [7/2000], Step [34/56], Loss: 0.095\n",
      "Epoch [7/2000], Step [35/56], Loss: 0.105\n",
      "Epoch [7/2000], Step [36/56], Loss: 0.100\n",
      "Epoch [7/2000], Step [37/56], Loss: 0.081\n",
      "Epoch [7/2000], Step [38/56], Loss: 0.085\n",
      "Epoch [7/2000], Step [39/56], Loss: 0.101\n",
      "Epoch [7/2000], Step [40/56], Loss: 0.097\n",
      "Epoch [7/2000], Step [41/56], Loss: 0.107\n",
      "Epoch [7/2000], Step [42/56], Loss: 0.114\n",
      "Epoch [7/2000], Step [43/56], Loss: 0.111\n",
      "Epoch [7/2000], Step [44/56], Loss: 0.106\n",
      "Epoch [7/2000], Step [45/56], Loss: 0.092\n",
      "Epoch [7/2000], Step [46/56], Loss: 0.111\n",
      "Epoch [7/2000], Step [47/56], Loss: 0.100\n",
      "Epoch [7/2000], Step [48/56], Loss: 0.107\n",
      "Epoch [7/2000], Step [49/56], Loss: 0.098\n",
      "Epoch [7/2000], Step [50/56], Loss: 0.077\n",
      "Epoch [7/2000], Step [51/56], Loss: 0.116\n",
      "Epoch [7/2000], Step [52/56], Loss: 0.088\n",
      "Epoch [7/2000], Step [53/56], Loss: 0.118\n",
      "Epoch [7/2000], Step [54/56], Loss: 0.111\n",
      "Epoch [7/2000], Step [55/56], Loss: 0.114\n",
      "Epoch [7/2000], Step [56/56], Loss: 0.115\n",
      "Epoch [7/2000], Accumulated_Loss: 0.105\n",
      "Epoch [8/2000], Step [1/56], Loss: 0.120\n",
      "Epoch [8/2000], Step [2/56], Loss: 0.095\n",
      "Epoch [8/2000], Step [3/56], Loss: 0.117\n",
      "Epoch [8/2000], Step [4/56], Loss: 0.118\n",
      "Epoch [8/2000], Step [5/56], Loss: 0.105\n",
      "Epoch [8/2000], Step [6/56], Loss: 0.093\n",
      "Epoch [8/2000], Step [7/56], Loss: 0.129\n",
      "Epoch [8/2000], Step [8/56], Loss: 0.094\n",
      "Epoch [8/2000], Step [9/56], Loss: 0.095\n",
      "Epoch [8/2000], Step [10/56], Loss: 0.091\n",
      "Epoch [8/2000], Step [11/56], Loss: 0.107\n",
      "Epoch [8/2000], Step [12/56], Loss: 0.146\n",
      "Epoch [8/2000], Step [13/56], Loss: 0.118\n",
      "Epoch [8/2000], Step [14/56], Loss: 0.099\n",
      "Epoch [8/2000], Step [15/56], Loss: 0.085\n",
      "Epoch [8/2000], Step [16/56], Loss: 0.119\n",
      "Epoch [8/2000], Step [17/56], Loss: 0.107\n",
      "Epoch [8/2000], Step [18/56], Loss: 0.117\n",
      "Epoch [8/2000], Step [19/56], Loss: 0.128\n",
      "Epoch [8/2000], Step [20/56], Loss: 0.100\n",
      "Epoch [8/2000], Step [21/56], Loss: 0.104\n",
      "Epoch [8/2000], Step [22/56], Loss: 0.101\n",
      "Epoch [8/2000], Step [23/56], Loss: 0.082\n",
      "Epoch [8/2000], Step [24/56], Loss: 0.102\n",
      "Epoch [8/2000], Step [25/56], Loss: 0.104\n",
      "Epoch [8/2000], Step [26/56], Loss: 0.108\n",
      "Epoch [8/2000], Step [27/56], Loss: 0.099\n",
      "Epoch [8/2000], Step [28/56], Loss: 0.083\n",
      "Epoch [8/2000], Step [29/56], Loss: 0.107\n",
      "Epoch [8/2000], Step [30/56], Loss: 0.082\n",
      "Epoch [8/2000], Step [31/56], Loss: 0.081\n",
      "Epoch [8/2000], Step [32/56], Loss: 0.074\n",
      "Epoch [8/2000], Step [33/56], Loss: 0.116\n",
      "Epoch [8/2000], Step [34/56], Loss: 0.116\n",
      "Epoch [8/2000], Step [35/56], Loss: 0.104\n",
      "Epoch [8/2000], Step [36/56], Loss: 0.101\n",
      "Epoch [8/2000], Step [37/56], Loss: 0.126\n",
      "Epoch [8/2000], Step [38/56], Loss: 0.106\n",
      "Epoch [8/2000], Step [39/56], Loss: 0.107\n",
      "Epoch [8/2000], Step [40/56], Loss: 0.123\n",
      "Epoch [8/2000], Step [41/56], Loss: 0.098\n",
      "Epoch [8/2000], Step [42/56], Loss: 0.123\n",
      "Epoch [8/2000], Step [43/56], Loss: 0.098\n",
      "Epoch [8/2000], Step [44/56], Loss: 0.100\n",
      "Epoch [8/2000], Step [45/56], Loss: 0.077\n",
      "Epoch [8/2000], Step [46/56], Loss: 0.107\n",
      "Epoch [8/2000], Step [47/56], Loss: 0.098\n",
      "Epoch [8/2000], Step [48/56], Loss: 0.089\n",
      "Epoch [8/2000], Step [49/56], Loss: 0.097\n",
      "Epoch [8/2000], Step [50/56], Loss: 0.093\n",
      "Epoch [8/2000], Step [51/56], Loss: 0.110\n",
      "Epoch [8/2000], Step [52/56], Loss: 0.112\n",
      "Epoch [8/2000], Step [53/56], Loss: 0.104\n",
      "Epoch [8/2000], Step [54/56], Loss: 0.088\n",
      "Epoch [8/2000], Step [55/56], Loss: 0.107\n",
      "Epoch [8/2000], Step [56/56], Loss: 0.107\n",
      "Epoch [8/2000], Accumulated_Loss: 0.104\n",
      "Epoch [9/2000], Step [1/56], Loss: 0.110\n",
      "Epoch [9/2000], Step [2/56], Loss: 0.103\n",
      "Epoch [9/2000], Step [3/56], Loss: 0.098\n",
      "Epoch [9/2000], Step [4/56], Loss: 0.112\n",
      "Epoch [9/2000], Step [5/56], Loss: 0.107\n",
      "Epoch [9/2000], Step [6/56], Loss: 0.083\n",
      "Epoch [9/2000], Step [7/56], Loss: 0.091\n",
      "Epoch [9/2000], Step [8/56], Loss: 0.110\n",
      "Epoch [9/2000], Step [9/56], Loss: 0.078\n",
      "Epoch [9/2000], Step [10/56], Loss: 0.083\n",
      "Epoch [9/2000], Step [11/56], Loss: 0.099\n",
      "Epoch [9/2000], Step [12/56], Loss: 0.106\n",
      "Epoch [9/2000], Step [13/56], Loss: 0.100\n",
      "Epoch [9/2000], Step [14/56], Loss: 0.115\n",
      "Epoch [9/2000], Step [15/56], Loss: 0.074\n",
      "Epoch [9/2000], Step [16/56], Loss: 0.106\n",
      "Epoch [9/2000], Step [17/56], Loss: 0.102\n",
      "Epoch [9/2000], Step [18/56], Loss: 0.114\n",
      "Epoch [9/2000], Step [19/56], Loss: 0.134\n",
      "Epoch [9/2000], Step [20/56], Loss: 0.112\n",
      "Epoch [9/2000], Step [21/56], Loss: 0.112\n",
      "Epoch [9/2000], Step [22/56], Loss: 0.096\n",
      "Epoch [9/2000], Step [23/56], Loss: 0.088\n",
      "Epoch [9/2000], Step [24/56], Loss: 0.089\n",
      "Epoch [9/2000], Step [25/56], Loss: 0.116\n",
      "Epoch [9/2000], Step [26/56], Loss: 0.087\n",
      "Epoch [9/2000], Step [27/56], Loss: 0.120\n",
      "Epoch [9/2000], Step [28/56], Loss: 0.095\n",
      "Epoch [9/2000], Step [29/56], Loss: 0.123\n",
      "Epoch [9/2000], Step [30/56], Loss: 0.121\n",
      "Epoch [9/2000], Step [31/56], Loss: 0.098\n",
      "Epoch [9/2000], Step [32/56], Loss: 0.101\n",
      "Epoch [9/2000], Step [33/56], Loss: 0.115\n",
      "Epoch [9/2000], Step [34/56], Loss: 0.097\n",
      "Epoch [9/2000], Step [35/56], Loss: 0.101\n",
      "Epoch [9/2000], Step [36/56], Loss: 0.086\n",
      "Epoch [9/2000], Step [37/56], Loss: 0.104\n",
      "Epoch [9/2000], Step [38/56], Loss: 0.080\n",
      "Epoch [9/2000], Step [39/56], Loss: 0.102\n",
      "Epoch [9/2000], Step [40/56], Loss: 0.097\n",
      "Epoch [9/2000], Step [41/56], Loss: 0.118\n",
      "Epoch [9/2000], Step [42/56], Loss: 0.102\n",
      "Epoch [9/2000], Step [43/56], Loss: 0.089\n",
      "Epoch [9/2000], Step [44/56], Loss: 0.108\n",
      "Epoch [9/2000], Step [45/56], Loss: 0.092\n",
      "Epoch [9/2000], Step [46/56], Loss: 0.084\n",
      "Epoch [9/2000], Step [47/56], Loss: 0.122\n",
      "Epoch [9/2000], Step [48/56], Loss: 0.117\n",
      "Epoch [9/2000], Step [49/56], Loss: 0.119\n",
      "Epoch [9/2000], Step [50/56], Loss: 0.123\n",
      "Epoch [9/2000], Step [51/56], Loss: 0.099\n",
      "Epoch [9/2000], Step [52/56], Loss: 0.100\n",
      "Epoch [9/2000], Step [53/56], Loss: 0.109\n",
      "Epoch [9/2000], Step [54/56], Loss: 0.105\n",
      "Epoch [9/2000], Step [55/56], Loss: 0.097\n",
      "Epoch [9/2000], Step [56/56], Loss: 0.091\n",
      "Epoch [9/2000], Accumulated_Loss: 0.102\n",
      "Epoch [10/2000], Step [1/56], Loss: 0.108\n",
      "Epoch [10/2000], Step [2/56], Loss: 0.094\n",
      "Epoch [10/2000], Step [3/56], Loss: 0.116\n",
      "Epoch [10/2000], Step [4/56], Loss: 0.109\n",
      "Epoch [10/2000], Step [5/56], Loss: 0.099\n",
      "Epoch [10/2000], Step [6/56], Loss: 0.078\n",
      "Epoch [10/2000], Step [7/56], Loss: 0.112\n",
      "Epoch [10/2000], Step [8/56], Loss: 0.102\n",
      "Epoch [10/2000], Step [9/56], Loss: 0.100\n",
      "Epoch [10/2000], Step [10/56], Loss: 0.104\n",
      "Epoch [10/2000], Step [11/56], Loss: 0.080\n",
      "Epoch [10/2000], Step [12/56], Loss: 0.099\n",
      "Epoch [10/2000], Step [13/56], Loss: 0.103\n",
      "Epoch [10/2000], Step [14/56], Loss: 0.099\n",
      "Epoch [10/2000], Step [15/56], Loss: 0.077\n",
      "Epoch [10/2000], Step [16/56], Loss: 0.083\n",
      "Epoch [10/2000], Step [17/56], Loss: 0.109\n",
      "Epoch [10/2000], Step [18/56], Loss: 0.118\n",
      "Epoch [10/2000], Step [19/56], Loss: 0.117\n",
      "Epoch [10/2000], Step [20/56], Loss: 0.108\n",
      "Epoch [10/2000], Step [21/56], Loss: 0.103\n",
      "Epoch [10/2000], Step [22/56], Loss: 0.127\n",
      "Epoch [10/2000], Step [23/56], Loss: 0.089\n",
      "Epoch [10/2000], Step [24/56], Loss: 0.116\n",
      "Epoch [10/2000], Step [25/56], Loss: 0.145\n",
      "Epoch [10/2000], Step [26/56], Loss: 0.095\n",
      "Epoch [10/2000], Step [27/56], Loss: 0.079\n",
      "Epoch [10/2000], Step [28/56], Loss: 0.131\n",
      "Epoch [10/2000], Step [29/56], Loss: 0.101\n",
      "Epoch [10/2000], Step [30/56], Loss: 0.116\n",
      "Epoch [10/2000], Step [31/56], Loss: 0.124\n",
      "Epoch [10/2000], Step [32/56], Loss: 0.117\n",
      "Epoch [10/2000], Step [33/56], Loss: 0.101\n",
      "Epoch [10/2000], Step [34/56], Loss: 0.107\n",
      "Epoch [10/2000], Step [35/56], Loss: 0.111\n",
      "Epoch [10/2000], Step [36/56], Loss: 0.080\n",
      "Epoch [10/2000], Step [37/56], Loss: 0.098\n",
      "Epoch [10/2000], Step [38/56], Loss: 0.088\n",
      "Epoch [10/2000], Step [39/56], Loss: 0.104\n",
      "Epoch [10/2000], Step [40/56], Loss: 0.081\n",
      "Epoch [10/2000], Step [41/56], Loss: 0.094\n",
      "Epoch [10/2000], Step [42/56], Loss: 0.093\n",
      "Epoch [10/2000], Step [43/56], Loss: 0.090\n",
      "Epoch [10/2000], Step [44/56], Loss: 0.090\n",
      "Epoch [10/2000], Step [45/56], Loss: 0.097\n",
      "Epoch [10/2000], Step [46/56], Loss: 0.128\n",
      "Epoch [10/2000], Step [47/56], Loss: 0.109\n",
      "Epoch [10/2000], Step [48/56], Loss: 0.113\n",
      "Epoch [10/2000], Step [49/56], Loss: 0.105\n",
      "Epoch [10/2000], Step [50/56], Loss: 0.094\n",
      "Epoch [10/2000], Step [51/56], Loss: 0.099\n",
      "Epoch [10/2000], Step [52/56], Loss: 0.133\n",
      "Epoch [10/2000], Step [53/56], Loss: 0.090\n",
      "Epoch [10/2000], Step [54/56], Loss: 0.111\n",
      "Epoch [10/2000], Step [55/56], Loss: 0.094\n",
      "Epoch [10/2000], Step [56/56], Loss: 0.089\n",
      "Epoch [10/2000], Accumulated_Loss: 0.103\n",
      "Epoch [11/2000], Step [1/56], Loss: 0.108\n",
      "Epoch [11/2000], Step [2/56], Loss: 0.119\n",
      "Epoch [11/2000], Step [3/56], Loss: 0.097\n",
      "Epoch [11/2000], Step [4/56], Loss: 0.097\n",
      "Epoch [11/2000], Step [5/56], Loss: 0.090\n",
      "Epoch [11/2000], Step [6/56], Loss: 0.102\n",
      "Epoch [11/2000], Step [7/56], Loss: 0.081\n",
      "Epoch [11/2000], Step [8/56], Loss: 0.110\n",
      "Epoch [11/2000], Step [9/56], Loss: 0.089\n",
      "Epoch [11/2000], Step [10/56], Loss: 0.102\n",
      "Epoch [11/2000], Step [11/56], Loss: 0.104\n",
      "Epoch [11/2000], Step [12/56], Loss: 0.089\n",
      "Epoch [11/2000], Step [13/56], Loss: 0.106\n",
      "Epoch [11/2000], Step [14/56], Loss: 0.099\n",
      "Epoch [11/2000], Step [15/56], Loss: 0.093\n",
      "Epoch [11/2000], Step [16/56], Loss: 0.109\n",
      "Epoch [11/2000], Step [17/56], Loss: 0.128\n",
      "Epoch [11/2000], Step [18/56], Loss: 0.107\n",
      "Epoch [11/2000], Step [19/56], Loss: 0.093\n",
      "Epoch [11/2000], Step [20/56], Loss: 0.093\n",
      "Epoch [11/2000], Step [21/56], Loss: 0.076\n",
      "Epoch [11/2000], Step [22/56], Loss: 0.086\n",
      "Epoch [11/2000], Step [23/56], Loss: 0.099\n",
      "Epoch [11/2000], Step [24/56], Loss: 0.102\n",
      "Epoch [11/2000], Step [25/56], Loss: 0.086\n",
      "Epoch [11/2000], Step [26/56], Loss: 0.094\n",
      "Epoch [11/2000], Step [27/56], Loss: 0.111\n",
      "Epoch [11/2000], Step [28/56], Loss: 0.103\n",
      "Epoch [11/2000], Step [29/56], Loss: 0.104\n",
      "Epoch [11/2000], Step [30/56], Loss: 0.088\n",
      "Epoch [11/2000], Step [31/56], Loss: 0.091\n",
      "Epoch [11/2000], Step [32/56], Loss: 0.086\n",
      "Epoch [11/2000], Step [33/56], Loss: 0.096\n",
      "Epoch [11/2000], Step [34/56], Loss: 0.083\n",
      "Epoch [11/2000], Step [35/56], Loss: 0.114\n",
      "Epoch [11/2000], Step [36/56], Loss: 0.101\n",
      "Epoch [11/2000], Step [37/56], Loss: 0.097\n",
      "Epoch [11/2000], Step [38/56], Loss: 0.108\n",
      "Epoch [11/2000], Step [39/56], Loss: 0.092\n",
      "Epoch [11/2000], Step [40/56], Loss: 0.103\n",
      "Epoch [11/2000], Step [41/56], Loss: 0.115\n",
      "Epoch [11/2000], Step [42/56], Loss: 0.077\n",
      "Epoch [11/2000], Step [43/56], Loss: 0.106\n",
      "Epoch [11/2000], Step [44/56], Loss: 0.126\n",
      "Epoch [11/2000], Step [45/56], Loss: 0.095\n",
      "Epoch [11/2000], Step [46/56], Loss: 0.090\n",
      "Epoch [11/2000], Step [47/56], Loss: 0.079\n",
      "Epoch [11/2000], Step [48/56], Loss: 0.105\n",
      "Epoch [11/2000], Step [49/56], Loss: 0.073\n",
      "Epoch [11/2000], Step [50/56], Loss: 0.110\n",
      "Epoch [11/2000], Step [51/56], Loss: 0.104\n",
      "Epoch [11/2000], Step [52/56], Loss: 0.113\n",
      "Epoch [11/2000], Step [53/56], Loss: 0.092\n",
      "Epoch [11/2000], Step [54/56], Loss: 0.092\n",
      "Epoch [11/2000], Step [55/56], Loss: 0.096\n",
      "Epoch [11/2000], Step [56/56], Loss: 0.099\n",
      "Epoch [11/2000], Accumulated_Loss: 0.098\n",
      "Epoch [12/2000], Step [1/56], Loss: 0.093\n",
      "Epoch [12/2000], Step [2/56], Loss: 0.092\n",
      "Epoch [12/2000], Step [3/56], Loss: 0.095\n",
      "Epoch [12/2000], Step [4/56], Loss: 0.095\n",
      "Epoch [12/2000], Step [5/56], Loss: 0.095\n",
      "Epoch [12/2000], Step [6/56], Loss: 0.105\n",
      "Epoch [12/2000], Step [7/56], Loss: 0.096\n",
      "Epoch [12/2000], Step [8/56], Loss: 0.077\n",
      "Epoch [12/2000], Step [9/56], Loss: 0.110\n",
      "Epoch [12/2000], Step [10/56], Loss: 0.088\n",
      "Epoch [12/2000], Step [11/56], Loss: 0.111\n",
      "Epoch [12/2000], Step [12/56], Loss: 0.111\n",
      "Epoch [12/2000], Step [13/56], Loss: 0.102\n",
      "Epoch [12/2000], Step [14/56], Loss: 0.103\n",
      "Epoch [12/2000], Step [15/56], Loss: 0.085\n",
      "Epoch [12/2000], Step [16/56], Loss: 0.088\n",
      "Epoch [12/2000], Step [17/56], Loss: 0.080\n",
      "Epoch [12/2000], Step [18/56], Loss: 0.092\n",
      "Epoch [12/2000], Step [19/56], Loss: 0.093\n",
      "Epoch [12/2000], Step [20/56], Loss: 0.094\n",
      "Epoch [12/2000], Step [21/56], Loss: 0.067\n",
      "Epoch [12/2000], Step [22/56], Loss: 0.091\n",
      "Epoch [12/2000], Step [23/56], Loss: 0.091\n",
      "Epoch [12/2000], Step [24/56], Loss: 0.103\n",
      "Epoch [12/2000], Step [25/56], Loss: 0.104\n",
      "Epoch [12/2000], Step [26/56], Loss: 0.091\n",
      "Epoch [12/2000], Step [27/56], Loss: 0.085\n",
      "Epoch [12/2000], Step [28/56], Loss: 0.107\n",
      "Epoch [12/2000], Step [29/56], Loss: 0.101\n",
      "Epoch [12/2000], Step [30/56], Loss: 0.103\n",
      "Epoch [12/2000], Step [31/56], Loss: 0.085\n",
      "Epoch [12/2000], Step [32/56], Loss: 0.074\n",
      "Epoch [12/2000], Step [33/56], Loss: 0.108\n",
      "Epoch [12/2000], Step [34/56], Loss: 0.100\n",
      "Epoch [12/2000], Step [35/56], Loss: 0.087\n",
      "Epoch [12/2000], Step [36/56], Loss: 0.079\n",
      "Epoch [12/2000], Step [37/56], Loss: 0.081\n",
      "Epoch [12/2000], Step [38/56], Loss: 0.072\n",
      "Epoch [12/2000], Step [39/56], Loss: 0.095\n",
      "Epoch [12/2000], Step [40/56], Loss: 0.108\n",
      "Epoch [12/2000], Step [41/56], Loss: 0.113\n",
      "Epoch [12/2000], Step [42/56], Loss: 0.082\n",
      "Epoch [12/2000], Step [43/56], Loss: 0.089\n",
      "Epoch [12/2000], Step [44/56], Loss: 0.098\n",
      "Epoch [12/2000], Step [45/56], Loss: 0.083\n",
      "Epoch [12/2000], Step [46/56], Loss: 0.087\n",
      "Epoch [12/2000], Step [47/56], Loss: 0.092\n",
      "Epoch [12/2000], Step [48/56], Loss: 0.085\n",
      "Epoch [12/2000], Step [49/56], Loss: 0.105\n",
      "Epoch [12/2000], Step [50/56], Loss: 0.117\n",
      "Epoch [12/2000], Step [51/56], Loss: 0.098\n",
      "Epoch [12/2000], Step [52/56], Loss: 0.089\n",
      "Epoch [12/2000], Step [53/56], Loss: 0.093\n",
      "Epoch [12/2000], Step [54/56], Loss: 0.079\n",
      "Epoch [12/2000], Step [55/56], Loss: 0.111\n",
      "Epoch [12/2000], Step [56/56], Loss: 0.093\n",
      "Epoch [12/2000], Accumulated_Loss: 0.094\n",
      "Epoch [13/2000], Step [1/56], Loss: 0.087\n",
      "Epoch [13/2000], Step [2/56], Loss: 0.086\n",
      "Epoch [13/2000], Step [3/56], Loss: 0.096\n",
      "Epoch [13/2000], Step [4/56], Loss: 0.092\n",
      "Epoch [13/2000], Step [5/56], Loss: 0.094\n",
      "Epoch [13/2000], Step [6/56], Loss: 0.123\n",
      "Epoch [13/2000], Step [7/56], Loss: 0.086\n",
      "Epoch [13/2000], Step [8/56], Loss: 0.096\n",
      "Epoch [13/2000], Step [9/56], Loss: 0.109\n",
      "Epoch [13/2000], Step [10/56], Loss: 0.087\n",
      "Epoch [13/2000], Step [11/56], Loss: 0.100\n",
      "Epoch [13/2000], Step [12/56], Loss: 0.094\n",
      "Epoch [13/2000], Step [13/56], Loss: 0.072\n",
      "Epoch [13/2000], Step [14/56], Loss: 0.101\n",
      "Epoch [13/2000], Step [15/56], Loss: 0.075\n",
      "Epoch [13/2000], Step [16/56], Loss: 0.088\n",
      "Epoch [13/2000], Step [17/56], Loss: 0.097\n",
      "Epoch [13/2000], Step [18/56], Loss: 0.116\n",
      "Epoch [13/2000], Step [19/56], Loss: 0.082\n",
      "Epoch [13/2000], Step [20/56], Loss: 0.087\n",
      "Epoch [13/2000], Step [21/56], Loss: 0.106\n",
      "Epoch [13/2000], Step [22/56], Loss: 0.086\n",
      "Epoch [13/2000], Step [23/56], Loss: 0.085\n",
      "Epoch [13/2000], Step [24/56], Loss: 0.087\n",
      "Epoch [13/2000], Step [25/56], Loss: 0.100\n",
      "Epoch [13/2000], Step [26/56], Loss: 0.097\n",
      "Epoch [13/2000], Step [27/56], Loss: 0.099\n",
      "Epoch [13/2000], Step [28/56], Loss: 0.100\n",
      "Epoch [13/2000], Step [29/56], Loss: 0.088\n",
      "Epoch [13/2000], Step [30/56], Loss: 0.079\n",
      "Epoch [13/2000], Step [31/56], Loss: 0.099\n",
      "Epoch [13/2000], Step [32/56], Loss: 0.097\n",
      "Epoch [13/2000], Step [33/56], Loss: 0.082\n",
      "Epoch [13/2000], Step [34/56], Loss: 0.089\n",
      "Epoch [13/2000], Step [35/56], Loss: 0.119\n",
      "Epoch [13/2000], Step [36/56], Loss: 0.092\n",
      "Epoch [13/2000], Step [37/56], Loss: 0.092\n",
      "Epoch [13/2000], Step [38/56], Loss: 0.112\n",
      "Epoch [13/2000], Step [39/56], Loss: 0.102\n",
      "Epoch [13/2000], Step [40/56], Loss: 0.104\n",
      "Epoch [13/2000], Step [41/56], Loss: 0.092\n",
      "Epoch [13/2000], Step [42/56], Loss: 0.069\n",
      "Epoch [13/2000], Step [43/56], Loss: 0.083\n",
      "Epoch [13/2000], Step [44/56], Loss: 0.094\n",
      "Epoch [13/2000], Step [45/56], Loss: 0.099\n",
      "Epoch [13/2000], Step [46/56], Loss: 0.075\n",
      "Epoch [13/2000], Step [47/56], Loss: 0.106\n",
      "Epoch [13/2000], Step [48/56], Loss: 0.092\n",
      "Epoch [13/2000], Step [49/56], Loss: 0.114\n",
      "Epoch [13/2000], Step [50/56], Loss: 0.098\n",
      "Epoch [13/2000], Step [51/56], Loss: 0.098\n",
      "Epoch [13/2000], Step [52/56], Loss: 0.120\n",
      "Epoch [13/2000], Step [53/56], Loss: 0.075\n",
      "Epoch [13/2000], Step [54/56], Loss: 0.097\n",
      "Epoch [13/2000], Step [55/56], Loss: 0.078\n",
      "Epoch [13/2000], Step [56/56], Loss: 0.085\n",
      "Epoch [13/2000], Accumulated_Loss: 0.094\n",
      "Epoch [14/2000], Step [1/56], Loss: 0.133\n",
      "Epoch [14/2000], Step [2/56], Loss: 0.089\n",
      "Epoch [14/2000], Step [3/56], Loss: 0.088\n",
      "Epoch [14/2000], Step [4/56], Loss: 0.087\n",
      "Epoch [14/2000], Step [5/56], Loss: 0.087\n",
      "Epoch [14/2000], Step [6/56], Loss: 0.101\n",
      "Epoch [14/2000], Step [7/56], Loss: 0.092\n",
      "Epoch [14/2000], Step [8/56], Loss: 0.099\n",
      "Epoch [14/2000], Step [9/56], Loss: 0.088\n",
      "Epoch [14/2000], Step [10/56], Loss: 0.094\n",
      "Epoch [14/2000], Step [11/56], Loss: 0.115\n",
      "Epoch [14/2000], Step [12/56], Loss: 0.093\n",
      "Epoch [14/2000], Step [13/56], Loss: 0.086\n",
      "Epoch [14/2000], Step [14/56], Loss: 0.070\n",
      "Epoch [14/2000], Step [15/56], Loss: 0.103\n",
      "Epoch [14/2000], Step [16/56], Loss: 0.124\n",
      "Epoch [14/2000], Step [17/56], Loss: 0.076\n",
      "Epoch [14/2000], Step [18/56], Loss: 0.084\n",
      "Epoch [14/2000], Step [19/56], Loss: 0.091\n",
      "Epoch [14/2000], Step [20/56], Loss: 0.093\n",
      "Epoch [14/2000], Step [21/56], Loss: 0.088\n",
      "Epoch [14/2000], Step [22/56], Loss: 0.093\n",
      "Epoch [14/2000], Step [23/56], Loss: 0.099\n",
      "Epoch [14/2000], Step [24/56], Loss: 0.109\n",
      "Epoch [14/2000], Step [25/56], Loss: 0.083\n",
      "Epoch [14/2000], Step [26/56], Loss: 0.076\n",
      "Epoch [14/2000], Step [27/56], Loss: 0.102\n",
      "Epoch [14/2000], Step [28/56], Loss: 0.065\n",
      "Epoch [14/2000], Step [29/56], Loss: 0.065\n",
      "Epoch [14/2000], Step [30/56], Loss: 0.092\n",
      "Epoch [14/2000], Step [31/56], Loss: 0.121\n",
      "Epoch [14/2000], Step [32/56], Loss: 0.088\n",
      "Epoch [14/2000], Step [33/56], Loss: 0.083\n",
      "Epoch [14/2000], Step [34/56], Loss: 0.077\n",
      "Epoch [14/2000], Step [35/56], Loss: 0.092\n",
      "Epoch [14/2000], Step [36/56], Loss: 0.093\n",
      "Epoch [14/2000], Step [37/56], Loss: 0.099\n",
      "Epoch [14/2000], Step [38/56], Loss: 0.083\n",
      "Epoch [14/2000], Step [39/56], Loss: 0.094\n",
      "Epoch [14/2000], Step [40/56], Loss: 0.084\n",
      "Epoch [14/2000], Step [41/56], Loss: 0.111\n",
      "Epoch [14/2000], Step [42/56], Loss: 0.097\n",
      "Epoch [14/2000], Step [43/56], Loss: 0.117\n",
      "Epoch [14/2000], Step [44/56], Loss: 0.094\n",
      "Epoch [14/2000], Step [45/56], Loss: 0.097\n",
      "Epoch [14/2000], Step [46/56], Loss: 0.089\n",
      "Epoch [14/2000], Step [47/56], Loss: 0.109\n",
      "Epoch [14/2000], Step [48/56], Loss: 0.090\n",
      "Epoch [14/2000], Step [49/56], Loss: 0.097\n",
      "Epoch [14/2000], Step [50/56], Loss: 0.101\n",
      "Epoch [14/2000], Step [51/56], Loss: 0.086\n",
      "Epoch [14/2000], Step [52/56], Loss: 0.082\n",
      "Epoch [14/2000], Step [53/56], Loss: 0.077\n",
      "Epoch [14/2000], Step [54/56], Loss: 0.081\n",
      "Epoch [14/2000], Step [55/56], Loss: 0.080\n",
      "Epoch [14/2000], Step [56/56], Loss: 0.105\n",
      "Epoch [14/2000], Accumulated_Loss: 0.093\n",
      "Epoch [15/2000], Step [1/56], Loss: 0.091\n",
      "Epoch [15/2000], Step [2/56], Loss: 0.081\n",
      "Epoch [15/2000], Step [3/56], Loss: 0.080\n",
      "Epoch [15/2000], Step [4/56], Loss: 0.091\n",
      "Epoch [15/2000], Step [5/56], Loss: 0.118\n",
      "Epoch [15/2000], Step [6/56], Loss: 0.110\n",
      "Epoch [15/2000], Step [7/56], Loss: 0.095\n",
      "Epoch [15/2000], Step [8/56], Loss: 0.079\n",
      "Epoch [15/2000], Step [9/56], Loss: 0.094\n",
      "Epoch [15/2000], Step [10/56], Loss: 0.099\n",
      "Epoch [15/2000], Step [11/56], Loss: 0.089\n",
      "Epoch [15/2000], Step [12/56], Loss: 0.089\n",
      "Epoch [15/2000], Step [13/56], Loss: 0.076\n",
      "Epoch [15/2000], Step [14/56], Loss: 0.100\n",
      "Epoch [15/2000], Step [15/56], Loss: 0.091\n",
      "Epoch [15/2000], Step [16/56], Loss: 0.103\n",
      "Epoch [15/2000], Step [17/56], Loss: 0.080\n",
      "Epoch [15/2000], Step [18/56], Loss: 0.086\n",
      "Epoch [15/2000], Step [19/56], Loss: 0.101\n",
      "Epoch [15/2000], Step [20/56], Loss: 0.102\n",
      "Epoch [15/2000], Step [21/56], Loss: 0.119\n",
      "Epoch [15/2000], Step [22/56], Loss: 0.086\n",
      "Epoch [15/2000], Step [23/56], Loss: 0.082\n",
      "Epoch [15/2000], Step [24/56], Loss: 0.097\n",
      "Epoch [15/2000], Step [25/56], Loss: 0.097\n",
      "Epoch [15/2000], Step [26/56], Loss: 0.094\n",
      "Epoch [15/2000], Step [27/56], Loss: 0.105\n",
      "Epoch [15/2000], Step [28/56], Loss: 0.098\n",
      "Epoch [15/2000], Step [29/56], Loss: 0.120\n",
      "Epoch [15/2000], Step [30/56], Loss: 0.080\n",
      "Epoch [15/2000], Step [31/56], Loss: 0.101\n",
      "Epoch [15/2000], Step [32/56], Loss: 0.101\n",
      "Epoch [15/2000], Step [33/56], Loss: 0.095\n",
      "Epoch [15/2000], Step [34/56], Loss: 0.093\n",
      "Epoch [15/2000], Step [35/56], Loss: 0.100\n",
      "Epoch [15/2000], Step [36/56], Loss: 0.080\n",
      "Epoch [15/2000], Step [37/56], Loss: 0.112\n",
      "Epoch [15/2000], Step [38/56], Loss: 0.116\n",
      "Epoch [15/2000], Step [39/56], Loss: 0.090\n",
      "Epoch [15/2000], Step [40/56], Loss: 0.098\n",
      "Epoch [15/2000], Step [41/56], Loss: 0.082\n",
      "Epoch [15/2000], Step [42/56], Loss: 0.083\n",
      "Epoch [15/2000], Step [43/56], Loss: 0.096\n",
      "Epoch [15/2000], Step [44/56], Loss: 0.113\n",
      "Epoch [15/2000], Step [45/56], Loss: 0.122\n",
      "Epoch [15/2000], Step [46/56], Loss: 0.090\n",
      "Epoch [15/2000], Step [47/56], Loss: 0.098\n",
      "Epoch [15/2000], Step [48/56], Loss: 0.091\n",
      "Epoch [15/2000], Step [49/56], Loss: 0.098\n",
      "Epoch [15/2000], Step [50/56], Loss: 0.121\n",
      "Epoch [15/2000], Step [51/56], Loss: 0.094\n",
      "Epoch [15/2000], Step [52/56], Loss: 0.116\n",
      "Epoch [15/2000], Step [53/56], Loss: 0.083\n",
      "Epoch [15/2000], Step [54/56], Loss: 0.107\n",
      "Epoch [15/2000], Step [55/56], Loss: 0.106\n",
      "Epoch [15/2000], Step [56/56], Loss: 0.086\n",
      "Epoch [15/2000], Accumulated_Loss: 0.097\n",
      "Epoch [16/2000], Step [1/56], Loss: 0.107\n",
      "Epoch [16/2000], Step [2/56], Loss: 0.101\n",
      "Epoch [16/2000], Step [3/56], Loss: 0.097\n",
      "Epoch [16/2000], Step [4/56], Loss: 0.076\n",
      "Epoch [16/2000], Step [5/56], Loss: 0.103\n",
      "Epoch [16/2000], Step [6/56], Loss: 0.066\n",
      "Epoch [16/2000], Step [7/56], Loss: 0.077\n",
      "Epoch [16/2000], Step [8/56], Loss: 0.101\n",
      "Epoch [16/2000], Step [9/56], Loss: 0.087\n",
      "Epoch [16/2000], Step [10/56], Loss: 0.080\n",
      "Epoch [16/2000], Step [11/56], Loss: 0.092\n",
      "Epoch [16/2000], Step [12/56], Loss: 0.098\n",
      "Epoch [16/2000], Step [13/56], Loss: 0.083\n",
      "Epoch [16/2000], Step [14/56], Loss: 0.083\n",
      "Epoch [16/2000], Step [15/56], Loss: 0.095\n",
      "Epoch [16/2000], Step [16/56], Loss: 0.083\n",
      "Epoch [16/2000], Step [17/56], Loss: 0.102\n",
      "Epoch [16/2000], Step [18/56], Loss: 0.088\n",
      "Epoch [16/2000], Step [19/56], Loss: 0.079\n",
      "Epoch [16/2000], Step [20/56], Loss: 0.079\n",
      "Epoch [16/2000], Step [21/56], Loss: 0.084\n",
      "Epoch [16/2000], Step [22/56], Loss: 0.086\n",
      "Epoch [16/2000], Step [23/56], Loss: 0.080\n",
      "Epoch [16/2000], Step [24/56], Loss: 0.093\n",
      "Epoch [16/2000], Step [25/56], Loss: 0.092\n",
      "Epoch [16/2000], Step [26/56], Loss: 0.070\n",
      "Epoch [16/2000], Step [27/56], Loss: 0.081\n",
      "Epoch [16/2000], Step [28/56], Loss: 0.081\n",
      "Epoch [16/2000], Step [29/56], Loss: 0.091\n",
      "Epoch [16/2000], Step [30/56], Loss: 0.083\n",
      "Epoch [16/2000], Step [31/56], Loss: 0.109\n",
      "Epoch [16/2000], Step [32/56], Loss: 0.079\n",
      "Epoch [16/2000], Step [33/56], Loss: 0.090\n",
      "Epoch [16/2000], Step [34/56], Loss: 0.088\n",
      "Epoch [16/2000], Step [35/56], Loss: 0.093\n",
      "Epoch [16/2000], Step [36/56], Loss: 0.092\n",
      "Epoch [16/2000], Step [37/56], Loss: 0.097\n",
      "Epoch [16/2000], Step [38/56], Loss: 0.115\n",
      "Epoch [16/2000], Step [39/56], Loss: 0.093\n",
      "Epoch [16/2000], Step [40/56], Loss: 0.088\n",
      "Epoch [16/2000], Step [41/56], Loss: 0.107\n",
      "Epoch [16/2000], Step [42/56], Loss: 0.096\n",
      "Epoch [16/2000], Step [43/56], Loss: 0.089\n",
      "Epoch [16/2000], Step [44/56], Loss: 0.121\n",
      "Epoch [16/2000], Step [45/56], Loss: 0.097\n",
      "Epoch [16/2000], Step [46/56], Loss: 0.094\n",
      "Epoch [16/2000], Step [47/56], Loss: 0.080\n",
      "Epoch [16/2000], Step [48/56], Loss: 0.099\n",
      "Epoch [16/2000], Step [49/56], Loss: 0.094\n",
      "Epoch [16/2000], Step [50/56], Loss: 0.053\n",
      "Epoch [16/2000], Step [51/56], Loss: 0.104\n",
      "Epoch [16/2000], Step [52/56], Loss: 0.079\n",
      "Epoch [16/2000], Step [53/56], Loss: 0.102\n",
      "Epoch [16/2000], Step [54/56], Loss: 0.093\n",
      "Epoch [16/2000], Step [55/56], Loss: 0.075\n",
      "Epoch [16/2000], Step [56/56], Loss: 0.072\n",
      "Epoch [16/2000], Accumulated_Loss: 0.090\n",
      "Epoch [17/2000], Step [1/56], Loss: 0.096\n",
      "Epoch [17/2000], Step [2/56], Loss: 0.082\n",
      "Epoch [17/2000], Step [3/56], Loss: 0.086\n",
      "Epoch [17/2000], Step [4/56], Loss: 0.109\n",
      "Epoch [17/2000], Step [5/56], Loss: 0.110\n",
      "Epoch [17/2000], Step [6/56], Loss: 0.086\n",
      "Epoch [17/2000], Step [7/56], Loss: 0.114\n",
      "Epoch [17/2000], Step [8/56], Loss: 0.079\n",
      "Epoch [17/2000], Step [9/56], Loss: 0.103\n",
      "Epoch [17/2000], Step [10/56], Loss: 0.112\n",
      "Epoch [17/2000], Step [11/56], Loss: 0.076\n",
      "Epoch [17/2000], Step [12/56], Loss: 0.098\n",
      "Epoch [17/2000], Step [13/56], Loss: 0.103\n",
      "Epoch [17/2000], Step [14/56], Loss: 0.082\n",
      "Epoch [17/2000], Step [15/56], Loss: 0.087\n",
      "Epoch [17/2000], Step [16/56], Loss: 0.083\n",
      "Epoch [17/2000], Step [17/56], Loss: 0.093\n",
      "Epoch [17/2000], Step [18/56], Loss: 0.082\n",
      "Epoch [17/2000], Step [19/56], Loss: 0.105\n",
      "Epoch [17/2000], Step [20/56], Loss: 0.085\n",
      "Epoch [17/2000], Step [21/56], Loss: 0.077\n",
      "Epoch [17/2000], Step [22/56], Loss: 0.102\n",
      "Epoch [17/2000], Step [23/56], Loss: 0.085\n",
      "Epoch [17/2000], Step [24/56], Loss: 0.100\n",
      "Epoch [17/2000], Step [25/56], Loss: 0.086\n",
      "Epoch [17/2000], Step [26/56], Loss: 0.104\n",
      "Epoch [17/2000], Step [27/56], Loss: 0.084\n",
      "Epoch [17/2000], Step [28/56], Loss: 0.107\n",
      "Epoch [17/2000], Step [29/56], Loss: 0.092\n",
      "Epoch [17/2000], Step [30/56], Loss: 0.073\n",
      "Epoch [17/2000], Step [31/56], Loss: 0.098\n",
      "Epoch [17/2000], Step [32/56], Loss: 0.086\n",
      "Epoch [17/2000], Step [33/56], Loss: 0.114\n",
      "Epoch [17/2000], Step [34/56], Loss: 0.087\n",
      "Epoch [17/2000], Step [35/56], Loss: 0.079\n",
      "Epoch [17/2000], Step [36/56], Loss: 0.077\n",
      "Epoch [17/2000], Step [37/56], Loss: 0.083\n",
      "Epoch [17/2000], Step [38/56], Loss: 0.085\n",
      "Epoch [17/2000], Step [39/56], Loss: 0.097\n",
      "Epoch [17/2000], Step [40/56], Loss: 0.090\n",
      "Epoch [17/2000], Step [41/56], Loss: 0.101\n",
      "Epoch [17/2000], Step [42/56], Loss: 0.091\n",
      "Epoch [17/2000], Step [43/56], Loss: 0.107\n",
      "Epoch [17/2000], Step [44/56], Loss: 0.093\n",
      "Epoch [17/2000], Step [45/56], Loss: 0.095\n",
      "Epoch [17/2000], Step [46/56], Loss: 0.090\n",
      "Epoch [17/2000], Step [47/56], Loss: 0.098\n",
      "Epoch [17/2000], Step [48/56], Loss: 0.109\n",
      "Epoch [17/2000], Step [49/56], Loss: 0.080\n",
      "Epoch [17/2000], Step [50/56], Loss: 0.095\n",
      "Epoch [17/2000], Step [51/56], Loss: 0.072\n",
      "Epoch [17/2000], Step [52/56], Loss: 0.085\n",
      "Epoch [17/2000], Step [53/56], Loss: 0.097\n",
      "Epoch [17/2000], Step [54/56], Loss: 0.086\n",
      "Epoch [17/2000], Step [55/56], Loss: 0.106\n",
      "Epoch [17/2000], Step [56/56], Loss: 0.085\n",
      "Epoch [17/2000], Accumulated_Loss: 0.092\n",
      "Epoch [18/2000], Step [1/56], Loss: 0.067\n",
      "Epoch [18/2000], Step [2/56], Loss: 0.119\n",
      "Epoch [18/2000], Step [3/56], Loss: 0.093\n",
      "Epoch [18/2000], Step [4/56], Loss: 0.083\n",
      "Epoch [18/2000], Step [5/56], Loss: 0.079\n",
      "Epoch [18/2000], Step [6/56], Loss: 0.087\n",
      "Epoch [18/2000], Step [7/56], Loss: 0.089\n",
      "Epoch [18/2000], Step [8/56], Loss: 0.083\n",
      "Epoch [18/2000], Step [9/56], Loss: 0.072\n",
      "Epoch [18/2000], Step [10/56], Loss: 0.071\n",
      "Epoch [18/2000], Step [11/56], Loss: 0.102\n",
      "Epoch [18/2000], Step [12/56], Loss: 0.088\n",
      "Epoch [18/2000], Step [13/56], Loss: 0.105\n",
      "Epoch [18/2000], Step [14/56], Loss: 0.083\n",
      "Epoch [18/2000], Step [15/56], Loss: 0.111\n",
      "Epoch [18/2000], Step [16/56], Loss: 0.083\n",
      "Epoch [18/2000], Step [17/56], Loss: 0.094\n",
      "Epoch [18/2000], Step [18/56], Loss: 0.088\n",
      "Epoch [18/2000], Step [19/56], Loss: 0.079\n",
      "Epoch [18/2000], Step [20/56], Loss: 0.098\n",
      "Epoch [18/2000], Step [21/56], Loss: 0.086\n",
      "Epoch [18/2000], Step [22/56], Loss: 0.080\n",
      "Epoch [18/2000], Step [23/56], Loss: 0.112\n",
      "Epoch [18/2000], Step [24/56], Loss: 0.103\n",
      "Epoch [18/2000], Step [25/56], Loss: 0.081\n",
      "Epoch [18/2000], Step [26/56], Loss: 0.095\n",
      "Epoch [18/2000], Step [27/56], Loss: 0.094\n",
      "Epoch [18/2000], Step [28/56], Loss: 0.071\n",
      "Epoch [18/2000], Step [29/56], Loss: 0.098\n",
      "Epoch [18/2000], Step [30/56], Loss: 0.098\n",
      "Epoch [18/2000], Step [31/56], Loss: 0.097\n",
      "Epoch [18/2000], Step [32/56], Loss: 0.097\n",
      "Epoch [18/2000], Step [33/56], Loss: 0.065\n",
      "Epoch [18/2000], Step [34/56], Loss: 0.091\n",
      "Epoch [18/2000], Step [35/56], Loss: 0.086\n",
      "Epoch [18/2000], Step [36/56], Loss: 0.086\n",
      "Epoch [18/2000], Step [37/56], Loss: 0.097\n",
      "Epoch [18/2000], Step [38/56], Loss: 0.075\n",
      "Epoch [18/2000], Step [39/56], Loss: 0.117\n",
      "Epoch [18/2000], Step [40/56], Loss: 0.083\n",
      "Epoch [18/2000], Step [41/56], Loss: 0.092\n",
      "Epoch [18/2000], Step [42/56], Loss: 0.108\n",
      "Epoch [18/2000], Step [43/56], Loss: 0.071\n",
      "Epoch [18/2000], Step [44/56], Loss: 0.079\n",
      "Epoch [18/2000], Step [45/56], Loss: 0.084\n",
      "Epoch [18/2000], Step [46/56], Loss: 0.096\n",
      "Epoch [18/2000], Step [47/56], Loss: 0.098\n",
      "Epoch [18/2000], Step [48/56], Loss: 0.099\n",
      "Epoch [18/2000], Step [49/56], Loss: 0.101\n",
      "Epoch [18/2000], Step [50/56], Loss: 0.085\n",
      "Epoch [18/2000], Step [51/56], Loss: 0.081\n",
      "Epoch [18/2000], Step [52/56], Loss: 0.072\n",
      "Epoch [18/2000], Step [53/56], Loss: 0.085\n",
      "Epoch [18/2000], Step [54/56], Loss: 0.064\n",
      "Epoch [18/2000], Step [55/56], Loss: 0.089\n",
      "Epoch [18/2000], Step [56/56], Loss: 0.084\n",
      "Epoch [18/2000], Accumulated_Loss: 0.089\n",
      "Epoch [19/2000], Step [1/56], Loss: 0.083\n",
      "Epoch [19/2000], Step [2/56], Loss: 0.086\n",
      "Epoch [19/2000], Step [3/56], Loss: 0.113\n",
      "Epoch [19/2000], Step [4/56], Loss: 0.105\n",
      "Epoch [19/2000], Step [5/56], Loss: 0.099\n",
      "Epoch [19/2000], Step [6/56], Loss: 0.119\n",
      "Epoch [19/2000], Step [7/56], Loss: 0.107\n",
      "Epoch [19/2000], Step [8/56], Loss: 0.077\n",
      "Epoch [19/2000], Step [9/56], Loss: 0.105\n",
      "Epoch [19/2000], Step [10/56], Loss: 0.108\n",
      "Epoch [19/2000], Step [11/56], Loss: 0.072\n",
      "Epoch [19/2000], Step [12/56], Loss: 0.087\n",
      "Epoch [19/2000], Step [13/56], Loss: 0.091\n",
      "Epoch [19/2000], Step [14/56], Loss: 0.093\n",
      "Epoch [19/2000], Step [15/56], Loss: 0.101\n",
      "Epoch [19/2000], Step [16/56], Loss: 0.080\n",
      "Epoch [19/2000], Step [17/56], Loss: 0.080\n",
      "Epoch [19/2000], Step [18/56], Loss: 0.086\n",
      "Epoch [19/2000], Step [19/56], Loss: 0.076\n",
      "Epoch [19/2000], Step [20/56], Loss: 0.086\n",
      "Epoch [19/2000], Step [21/56], Loss: 0.085\n",
      "Epoch [19/2000], Step [22/56], Loss: 0.099\n",
      "Epoch [19/2000], Step [23/56], Loss: 0.096\n",
      "Epoch [19/2000], Step [24/56], Loss: 0.121\n",
      "Epoch [19/2000], Step [25/56], Loss: 0.109\n",
      "Epoch [19/2000], Step [26/56], Loss: 0.073\n",
      "Epoch [19/2000], Step [27/56], Loss: 0.080\n",
      "Epoch [19/2000], Step [28/56], Loss: 0.095\n",
      "Epoch [19/2000], Step [29/56], Loss: 0.100\n",
      "Epoch [19/2000], Step [30/56], Loss: 0.090\n",
      "Epoch [19/2000], Step [31/56], Loss: 0.083\n",
      "Epoch [19/2000], Step [32/56], Loss: 0.109\n",
      "Epoch [19/2000], Step [33/56], Loss: 0.112\n",
      "Epoch [19/2000], Step [34/56], Loss: 0.093\n",
      "Epoch [19/2000], Step [35/56], Loss: 0.104\n",
      "Epoch [19/2000], Step [36/56], Loss: 0.100\n",
      "Epoch [19/2000], Step [37/56], Loss: 0.091\n",
      "Epoch [19/2000], Step [38/56], Loss: 0.080\n",
      "Epoch [19/2000], Step [39/56], Loss: 0.087\n",
      "Epoch [19/2000], Step [40/56], Loss: 0.087\n",
      "Epoch [19/2000], Step [41/56], Loss: 0.090\n",
      "Epoch [19/2000], Step [42/56], Loss: 0.081\n",
      "Epoch [19/2000], Step [43/56], Loss: 0.107\n",
      "Epoch [19/2000], Step [44/56], Loss: 0.077\n",
      "Epoch [19/2000], Step [45/56], Loss: 0.092\n",
      "Epoch [19/2000], Step [46/56], Loss: 0.102\n",
      "Epoch [19/2000], Step [47/56], Loss: 0.084\n",
      "Epoch [19/2000], Step [48/56], Loss: 0.074\n",
      "Epoch [19/2000], Step [49/56], Loss: 0.114\n",
      "Epoch [19/2000], Step [50/56], Loss: 0.083\n",
      "Epoch [19/2000], Step [51/56], Loss: 0.090\n",
      "Epoch [19/2000], Step [52/56], Loss: 0.078\n",
      "Epoch [19/2000], Step [53/56], Loss: 0.093\n",
      "Epoch [19/2000], Step [54/56], Loss: 0.084\n",
      "Epoch [19/2000], Step [55/56], Loss: 0.086\n",
      "Epoch [19/2000], Step [56/56], Loss: 0.089\n",
      "Epoch [19/2000], Accumulated_Loss: 0.092\n",
      "Epoch [20/2000], Step [1/56], Loss: 0.105\n",
      "Epoch [20/2000], Step [2/56], Loss: 0.094\n",
      "Epoch [20/2000], Step [3/56], Loss: 0.098\n",
      "Epoch [20/2000], Step [4/56], Loss: 0.081\n",
      "Epoch [20/2000], Step [5/56], Loss: 0.085\n",
      "Epoch [20/2000], Step [6/56], Loss: 0.086\n",
      "Epoch [20/2000], Step [7/56], Loss: 0.087\n",
      "Epoch [20/2000], Step [8/56], Loss: 0.098\n",
      "Epoch [20/2000], Step [9/56], Loss: 0.103\n",
      "Epoch [20/2000], Step [10/56], Loss: 0.116\n",
      "Epoch [20/2000], Step [11/56], Loss: 0.089\n",
      "Epoch [20/2000], Step [12/56], Loss: 0.095\n",
      "Epoch [20/2000], Step [13/56], Loss: 0.081\n",
      "Epoch [20/2000], Step [14/56], Loss: 0.077\n",
      "Epoch [20/2000], Step [15/56], Loss: 0.093\n",
      "Epoch [20/2000], Step [16/56], Loss: 0.082\n",
      "Epoch [20/2000], Step [17/56], Loss: 0.088\n",
      "Epoch [20/2000], Step [18/56], Loss: 0.084\n",
      "Epoch [20/2000], Step [19/56], Loss: 0.082\n",
      "Epoch [20/2000], Step [20/56], Loss: 0.081\n",
      "Epoch [20/2000], Step [21/56], Loss: 0.092\n",
      "Epoch [20/2000], Step [22/56], Loss: 0.093\n",
      "Epoch [20/2000], Step [23/56], Loss: 0.078\n",
      "Epoch [20/2000], Step [24/56], Loss: 0.093\n",
      "Epoch [20/2000], Step [25/56], Loss: 0.091\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(checkpoint_epoch, EPOCHS):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# to keep track of accumulated accuracy\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     13\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m         t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m, T\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, (BATCH_SIZE,))\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/envs/deep-learning-3.9.8/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/envs/deep-learning-3.9.8/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/envs/deep-learning-3.9.8/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/envs/deep-learning-3.9.8/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/lib/python3.9/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/lib/python3.9/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/lib/python3.9/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/lib/python3.9/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.8/lib/python3.9/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training algorithm\n",
    "# x_0 ~ q(x_0)\n",
    "# t ~ Uniform({1, ...., T})\n",
    "# eps ~ N(0, I)\n",
    "# Take graident descent step on MSE(eps - eps_theta(root(alpha_t_bar)*x_0 + root(1-alpha_t_bar)*eps, t))\n",
    "# Until converged\n",
    "\n",
    "for epoch in range(checkpoint_epoch, EPOCHS):\n",
    "    # to keep track of accumulated accuracy\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for step, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        t = torch.randint(1, T+1, (BATCH_SIZE,)).to(device)\n",
    "        epsilon = torch.randn_like(images)\n",
    "\n",
    "        noised_samples = (sqrt_alphas_cumprod[t - 1][(...,) + (None,) * 3] * images) + (sqrt_one_minus_alphas_cumprod[t - 1][(...,) + (None,) * 3] * epsilon)\n",
    "        # [(...,) + (None,) * 3] is used so that I can turn the tensor of shape N to N 1 1 1, which allows me to multiply it with images and epsilon which have shape N C W H\n",
    "\n",
    "        # Forward pass\n",
    "        predicted_epsilon = model(noised_samples, t)\n",
    "        loss = criterion(predicted_epsilon, epsilon)\n",
    "        epoch_loss += loss\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step+1) % 1 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{EPOCHS}], Step [{step+1}/{len(dataloader)}], Loss: {loss.item():.3f}')\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Accumulated_Loss: {(epoch_loss.item() / (len(dataloader))):.3f}')\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'epoch': 28\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, './checkpoints/model_checkpoint_28.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGfCAYAAAAZGgYhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAABdL0lEQVR4nO29e5RdZZ3n/d17n2vdzqmqpC65EiCQAIZLgBDBaYVohlEHG8ZRF/0O0+Nql0ygBZzVbWa10s3qNoyuaWntEFqaAXu1dLqZd6GiI7RvlDBqgiSAcjMkEEjlUpVbXU/Vue293z/SVlt5vj/NIcFdKb+ftWot+NWTfZ59O0+d2p/6/rw4jmMIIYQQv2H8pCcghBDitxMtQEIIIRJBC5AQQohE0AIkhBAiEbQACSGESAQtQEIIIRJBC5AQQohE0AIkhBAiEbQACSGESAQtQEIIIRIh9XZteP369fjiF7+I/v5+XHjhhfjKV76Cyy+//Nf+uyiKsH//frS2tsLzvLdrekIIId4m4jjG6Ogo5syZA9//FZ9z4reBjRs3xplMJv5f/+t/xS+99FL8B3/wB3GxWIwHBgZ+7b/t6+uLAehLX/rSl75O86++vr5f+X7vxfGpDyNdsWIFLrvsMvz1X/81gGOfaubPn49bb70Vn/nMZ37lvx0eHkaxWERmfg6eP/UTUGVogv8jssB6AR8aW4txI/VGP5hZ4xv5BWhk1GtGvX7i27GmZ30AjYxja36ezriltrY2OrSzcxattxc7aL2pqZXWc015p5bPNfHpZbO0bn0Cj2J2MvjYwOcHK4pCWh8aHaL1/kMHnNqhQ/107MR4idat85zN5pxaazM/P+3FdlovtBb5tnPueQCAmEymFvKLuWTsz8GD+2h93549Tq0yVqVjrVswneXnrbXAj4ufcsePjI7SseVShb+odc9akCk2NfNrvLW5hdYrJf6eOjzgzj023n67Otx7M4piHD4wiKGhIRQKBf4P8Tb8Cq5arWL79u1Yu3btZM33faxatQpbtmxxxlcqFVQq/3pCRv/lpHm+5yxA1h3E3ic848o6bRcgi0b3h/y4YS5AxjYaPbbOeQTgBfxV/RTfSJDml2oqw+vpTNqtZd3asTpZIZHMApSq8DkGaXc71rGyjq15GZLtsNcDgJR5Hqxjy+tsAYqNN+BUnb+mb8yR7b95LfNyw9cnq1vbaPietaA/eDc2b/NaIcPZOQNg/Irt2D3y6x6jnHIJ4fDhwwjDEN3d3VPq3d3d6O93f2Jbt24dCoXC5Nf8+fNP9ZSEEEJMQxK34NauXYvh4eHJr76+vqSnJIQQ4jfAKf8V3KxZsxAEAQYGBqbUBwYG0NPT44zPZrPIkt/B1ys191c3xmxj8hsUVgMA8N8IAGnjozX5FUI6zTfCftUEWL+yAbI5/uyht8s9Tp2tnXTs6JERWj+wz31mAABHDh5y51c3HgOavyA36s38BBVnu78j7umZQ8fO7eWfgGfP7qb1pmb+DChDnmtkMvx4p1J8h0LjvFVr7vME60lqc76Z1rPGXMbL47Te1u/uf9O+N+jYkZFBWveNX4e0kmNYLPBnPZ3F2bReKBRpPW3sJztcfmD8OjXF64cO7af1H/34+07t1RdfoGND/ptQ5Nv4G8j5l1xE64sWnevUDh/l5+GNPW/Q+t797rMrADg6dJjWvcC9QecsOouOvejcd9B6ZYw/X3vmh087tQOv8veUg0fc+Z2oWXDKPwFlMhksX74cmzZtmqxFUYRNmzZh5cqVp/rlhBBCnKa8LX8HdMcdd+Cmm27CpZdeissvvxz33HMPSqUSfv/3f//teDkhhBCnIW/LAvSRj3wEhw4dwuc+9zn09/fjoosuwuOPP+6ICUIIIX57eduSEG655Rbccsstb9fmhRBCnOYkbsEJIYT47eRt+wR0ssRh6P7lkzXbrLuOZtv5XwQXu7hN1j67i9Zndbn1LsPIyhp/jHd4mFss1Rr/i+jLl17i1P7tVavp2MpwmdZ/vmMHrT/19A+d2k9/zg2hQ4OuMQcALUVunp1zzjm0ftbZrplTKBbp2FyenzernkoZuiP54zjPyKSy/liuQmw3ABgbH3NqYciNuZZWbpN1dfBrKGP8UezcMxc5tUWHl9Cx5TI3myybrCnvphWk03weaeN4W+Otv/Rkh6slz6+rM+acQesp448omwrudvYf4cbc2OEjtN5iXJ/vfc8HaP2GD/4/Tu3wkSE69rV9r9P6T3c9R+vbfv4MrR8+6r6vXLrkYjr2P6y6ntbzxl/o3p++z6n94/6v07HRCDmZSVlwQgghxImgBUgIIUQiaAESQgiRCFqAhBBCJMK0lRCiegzPP+5JlhEBk21x407ecZH7IB8AlpzPIylaO3ncfwtpG7B44dl0bLchJ7y6hwsBP9r2f2mdPaA97ww36gMAusCliqsvvJrW37XCrW/ZwR9+7trPH5ZmcvyB8/x582i9ucUVCEbHeYTQeJlnvltNraz4FiohWJHIRoRSaLSd8Eg6s29ICCkjbinTxKWKFiNaKNdC4nI6eCxObCRtWxICPPeJcbnC5RYWQ/SrsJ5F+5H7nXTGjU8CgOYsj/Of29FL61etXOXUvvej79Cxr5a4hBAafU58I/K9mHHn2NHL781FvTwu57wLLqT12c8voPWnf7bVqXXN5mOXLjqf1nvTRVrfdpErPvzj//sPdCzGJCEIIYQ4zdACJIQQIhG0AAkhhEgELUBCCCESQQuQEEKIRJi2FhxCwOkHZlhJixa6ETAfXP0f6Ngzly6l9b2DA7Q+PObaWi15bsydO/8CWs8SgwkAHt/y/9H6d59y6/NbeVTQx97D97O7yI28ZQtdm653/hl07Bsj/JgcGuHRQuMVHgEzNHbUqY2VeQxRvc4NLivmyAsMNZJZcKSB17GhRh3cbPPS7m0TGNdm3mhIl8m58TfHNsRvyYDUc9a+E8MMsBvS1ULSYM+r820TYw4AfOvYGvUU2Uxs3OClcW7koYPbmIvmue8HC88+k4599Q1ugI7WR2l98/NulBUA9M51t3/xOcvp2K5Obu8Vc/x9ZVan26ASALLk2oqNzxRxxI+tb2Ql5Vn0lXGN05eUBSeEEGI6owVICCFEImgBEkIIkQhagIQQQiSCFiAhhBCJMH0tuAikmRVfL89fvMypvfuq99KxfhvP4Hp9yLDgxl2zqzrBc6Ly4NvubecZaZk8z7565c2Xndq9G79Kxx7s76f1333PdbT+jiWumZP3eV5Za76F1vcP82N1cIjnag2PuBZcrcotuJyRB5bNcmvMN/LNIiL3BMbYVJrbZOHxWYT/gjfuXoe+kSfX1GxkvrVyMzKV4ucijN25RHVu6dXr3GCLQm4YgtSDgM/D84xj2OCxjYkmZe1Pzci246OBtqzbBHD+ArehHwAErXzeYYkfw2d3baP18v9xZ/Pq3p107JWX/A6tF7q56TpUGqJ1Zpq15NzsSgDwjUaCE4auNjjOXtM64m8dfQISQgiRCFqAhBBCJIIWICGEEImgBUgIIUQiaAESQgiRCNPXgmP43KhZONftUNo1m5tnBypDtD5sdOKE5wYg5TLcyEoZ63kO3OxqNUwoZFzbZOc+14wDgAe/M0Tr+49yU+2jH7jRqS066zw6drjG87CGRgZpfYzk5gFAtepagymPn8tchltjWeOYM9sNAAkSBLJpw7AzbMTQ4/ZVPXINqXJpnI4tjfNjaEhzaG3h5qFPct/CGreSKlY30yrvZhqQTrEpw3aLyXEFgMA3ssZSvF6P3WPLMukAwDMy7BDzetpzDb7WtiLfdprPL93E6+WI5x2+sNu14w6PckP14NhBWj9niWvzAsDAKLdLm8n13Nsxh45NBfwaPxzy63NX/2tu0ej6y87CCUbB6ROQEEKIZNACJIQQIhG0AAkhhEgELUBCCCESYfpKCB6cp1usARMA9HbPd2qpNH9oPTbMHwwaz9fQ3uY2iZrVMZsPNhip8YfzR4b5w0g/cCcT5PlD0YHRA7T++LZ/pvUo757yq8v/lo5tKrqRJgAwURrj267x+JKAPCzOpnnUS96I4kkbAkqVCAEAEJDuWbkU33azETlUCXlc0HjZFQ769r1Ox44Nc2EjrPEH7uecyYUQ1sQsnzX2J8NjVyqGhFAhMS01Y6wV5xOTqCAACK3H0XX32EaGVBAYTdOMpCSASQuGrRJaN77x1pgxGiPGgXsd9g/20bFbX/q/tD5U5+8TrW38Puxsdt+H5szizeusQ7X74B5af+WNn7tFI8mJ9iiMT0xE0CcgIYQQiaAFSAghRCJoARJCCJEIWoCEEEIkghYgIYQQiTB9LTgfjgXXVuA2SHfXXKcWGWtrucLtnqYsj4CZU3CbRHV3ctOEt6kDfvr6C7T+ymu8HpFIkrZWbmqFIXdNRo2IjW07tzu1lvYiHXv2GUv5axpWEiIjpoWci4xhtaWNiB7fOJ++YU55JF4mZzR7azKMyZGA3x7lCdeCO3h4Px27r49EmgDoP/Amre95cxetnzl/sVPrmc1jV1pbCrSeMfY/Q2J+mo3oI+s8VIwGg+NVHgtUr7nXbdow1TLEaARsCy4i/pUf8XnHXKJErc6VrzDN6wFpvBcZDdyOjvFonQOH99J6JsvPW3G223yu3TDmLIZHh2h9aMhtImlhpmGdwL/VJyAhhBCJoAVICCFEImgBEkIIkQhagIQQQiSCFiAhhBCJcFpZcO2WBddLLDhDwQiNb3S08m3PneVmcDVluZFWAjfstr36DK0f6OdZUcwq8Y0MqpRhakWGm1Ijrt7RMW68DI8N0XrayFSLjFytgDT1s5r3MWMOAFDn5803zKl0yj0uWY9npKWN5mtmnTRfi0KuU40YzcSGB3nDwL17eabcs81Fp9bZNouObW/t5PUWfo2zJmbnLjiXjj2jZxGtW6ZavcytsWrFPV7Wm1GeXD8AEBj3eEyu/ZYm1xgDgEya269Vo0FlybBoU8R48wJ+LafS/D6phdyjtRr1hWy8kcnHjyDQaRiTs8j74WuW7nYS6BOQEEKIRNACJIQQIhG0AAkhhEgELUBCCCESQQuQEEKIRGjYgnvqqafwxS9+Edu3b8eBAwfw6KOP4kMf+tDk9+M4xp133on7778fQ0NDuPLKK7FhwwYsXuxmWf1ajrMu8jmeTzW7x81rs3KIsqT7IwB0F3mX0+6iu22LPYM8y+m5ndtoPa7w/Kx8jlhmKe6xxEY9Z2R5tbYV3U2n+TGpGlZObP3cYuRtMVPPjxuz3azAqZSRBZchr5my8sCq3EpKx/zYzml3zcjB3jPo2MDoIHrkCO/MOzjEu+QeObTPqe22jlXN6CAacpOys8m15laev4KOff87/x2tz+9aQOsZIwsuQzqiBlk+v6bIsOCMDp0ReVfrncVz85aedSGt9/fzTsNpo5NvponUSR4hADRleWfnpmZu6o1XuZG3e/8bTm1fN+9werZxfhb3LKT1cxee49Se9p6iY5lYbMh4Dg1/AiqVSrjwwguxfv16+v0vfOEL+PKXv4z77rsPTz/9NJqbm7F69WqUy/xCFEII8dtJw5+Arr32Wlx77bX0e3Ec45577sGf/Mmf4LrrrgMA/N3f/R26u7vxjW98Ax/96Eedf1OpVFD5pU8CIyO8L7oQQoiZxSl9BrR792709/dj1apVk7VCoYAVK1Zgy5Yt9N+sW7cOhUJh8mv+/PmnckpCCCGmKad0AervP/Z77e7u7in17u7uye8dz9q1azE8PDz51dfH0wGEEELMLBKP4slms8gaDZeEEELMXE7pAtTTc6xT6MDAAHp7/9UUGhgYwEUXXdTYxjw45tP4OO/yOT4x7NTyXpGOnWXkyRXy3EwpNrvjrc6nL7z5Eq2/+vor/B8Ypkg25+ZT+cYizToxAkCrkW3X0eHafllj3+uWymJ0i0wb1g/Ld/MsS6Zu5ckZHVGNeo4YbEGVv2jkGfledW6TLex0f03cej7Px5tLctYA4LU3dtD6vv28U+rQUTc7bnjsEB1bGhqj9coIP2/9Vfc14yP8Xjsn303rHefytxKP5OYBQIbcRalmbm6mM1xginJ8P+sk3q2nhVuuq1a8j9aPDA/RejbPs+NyZO6xb3UO5uUJw3Z7Y99uWn/5jZedWm8zv97e/Y4raX2W30rrZ3Sd+KOQExTeKKf0V3CLFi1CT08PNm3aNFkbGRnB008/jZUrV57KlxJCCHGa0/AnoLGxMeza9a9963fv3o3nn38eHR0dWLBgAW677Tb8+Z//ORYvXoxFixbhs5/9LObMmTPlb4WEEEKIhhegbdu24T3vec/k/99xxx0AgJtuugkPPfQQ/uiP/gilUgmf+MQnMDQ0hKuuugqPP/44cuyPK4UQQvzW0vAC9O53vxvxr/gzV8/zcNddd+Guu+46qYkJIYSY2SRuwZn4cJ5QHTi8nw79zve+5dTe9Z5VZCTQ+S+ixPEUWztonTUfG4n4Q9EXdv+M1gcH+cPiVIaLBa1Fdy6WKJAyokFmt/MIoc4Od//TWf7w1+gvB98wCDzfEAXYo0ZDNoDHH5QHRhZPxnjQmyFl3+P6SFThdd9oMldIuw0JZ83m0sf84jxaP7PrTFrfe4A/cN7Tt9Op7drpPoQGgDdLfBu1Gm88yByMcLREx4ZHuZwQH3JFIAAIjKZ+KbjnOcgb0U8hvz6rEf+tykSTe33mjNii8+cupfX6PH5dBXk+l1STG2cVkKaIAOAb9f5B/qcqe4/w9729h9zor5+++hwdOzjM34M6jaixYpY3qjvVKIxUCCFEImgBEkIIkQhagIQQQiSCFiAhhBCJoAVICCFEIkxfCy4FZ3kcmxiiQ//P97/p1Ibq3Mq5/NJ30vo7zr2Y1vMtrmXWP8JtlR1vcisJETe+Zs/iRt5Zi5Y4NcuCi4yGbK1NvLlVK4kWSqW4SRd7RmMzI14lMOoem6NhwXmx0RzOCPzIGEZeukYsK6MBoJHmg6zH5+Jn3NsmByMSKe02ewOAji4egXKWYc0N9rrXxK5ZbtMwAPhpnptQO31+fdaH3Uib82ctomPnprjtlzVifuKqEXNELLhU3ohhinjkjhfze9wjhzbt83m0B9xqq+d4k8a6z98yo9C9iDIZbum1tnLDzOh/iLwR/zNeG3dqew/zppiHh7gFd7ZhwWXTDfzdppE4dCLoE5AQQohE0AIkhBAiEbQACSGESAQtQEIIIRJBC5AQQohEmLYWnBe4dlIUcNNmF2kEN/EjnmV1ZPQgrU+M8/GLiJH2XB/PfDtwgDcTa8kbxtOCc2n9nDPPd2p+ils542VudgU+t7Kyadf6SRkGVwDLdjPGe3y8H7majBdyey0VGVabEYCbNvLafNI0zzNMqIyRzZVNG/uTdecS1Pg8kDLmbWTYNftuzhwAzGpy63MX8iZrZwcLaH1P+3m0XjvsZsTNCfg12wtuwXmHeT5ibFyfLMMv02Q1FzTOm8cbuMVVd9sFo+ly0MSv5QnjWp4wlC/W69DPNJCNCACG0WrhBe74esSvw9EJ/v5mNdeMmRpqNdgzu0v+evQJSAghRCJoARJCCJEIWoCEEEIkghYgIYQQiaAFSAghRCJMWwsujoHj47+CgK+XHlzzY2x8kI7dtYfnYTGjBAB29r/mbuPw63RsdczNZgKA7nae+Tav5wxa7yTjDWkM6TQ3hDzjZ4sUMdiCiI8NiL0GACnjsgmMMCufzN0QGhHUeB5YyrLdysaBqRILzrD60mluGKazvB6QcmAYT3HA9ycCr9eJvQcAATmfxZAbXM1+L60vmM1zz6KMm6kWDBv22pDRDXeC57WlDTswTczDXJmranlyfwNACty88yvusYpa+bWcjfk5LlmdXI2uv1WSD5hO8eNNYvAAAGMlfgyrRp5eLuduv8nKjDQ+avAtA3WWvWhk1dG60fD4ePQJSAghRCJoARJCCJEIWoCEEEIkghYgIYQQiTBtJQREjoOA2IpjCdynYK1NvIlTKs3X3P1H+mi9v+Q2cjpacqNLACBnPLjsKnIJoa2lg9Z9jzwYNVIwmqwH5UZcjkc25BkPRb26FZdjNKqrn7jMkLKSawwJwSsbTzXHeZiIV3ZfwI+NB8tpowmeMcmAPEBPZw15wBAcjEOIWpXvT0SkirjC55ex4m+MYxWPuvsfDRnHdZTXgwrf/7ThiGTIN/Jlvu1cZMRNkYZsABCMu+fHn+DXZjbkT9YzxrWSDvh9VWlyz3MMQ3CoGnE5I6O0HhrNG9ua3cZ27W28AaJnXIcVo9FjjZlD1mpxEh9j9AlICCFEImgBEkIIkQhagIQQQiSCFiAhhBCJoAVICCFEIkxfC45E8USGlRUTSSSb5rEeuUyO1ishN3AmRtxGTtU6jwBpzvEYjGIrb+LFmsMBgE9OSyplNIEzrByrHoXuwYoNK8djBxZAyrARA8tSJMpXps41sKDMbSq/ZNX5efMn3LoV55MK+Lx9j88xJk3zQpbPAyDI8nOcNs4na6QHABE5R9EEt8PicaNR2xhvShaPuvXUGN92yjg/6ZphTBr6ZoY06svl+X2Vq3PbzedTpBZcMMGPd9q4DlNGzFHa5+8r5bz7mhNV/vP9UJXvZ7nE9zOX4u9Zs9rdhoSzil10bJDh864ZFlyUIseF1QCA3T8n2FtPn4CEEEIkghYgIYQQiaAFSAghRCJoARJCCJEIWoCEEEIkwvS14EK4YXBGVzaPdFtKG1aSpWfUalypqdZJ3ePzyBnGUz7bQuvpgJsp6ZRbz2atbDu+Detni3rNbUEVhUajv9BqjmbYcREfz4y3dNUw6SYatN0MWysYd62xoM5zvwLjfPqG1RdW3LkYIiFgGExZ41phjecAICB2XFzmNlVk2FTxKG94Fo+641MTvFVZxuhgljHC7QLjfkun3GsoO873PV02rk/rmmh2732/xO8Tv2JkD9aMefv8fYXJtZGRJRjF3FL0jLkU827m27FvuNdn0ciCCwzr0oiBBNJkaTByNOGfYPc59k/f8r8UQgghTgItQEIIIRJBC5AQQohE0AIkhBAiEbQACSGESITTzILjQ1O+uxsZYpIBQBhxY6NaMzpRxu6LBswQAZDNcLMpm+YmVMYwpHIZ13jL51rpWMtuqRsGW0w6jvqGBca6pwJAEHO7J01y5gAgTQ5tumxYcMReAwC/xPWrwDChWN2vGTYVOccA4BvWZVwmJiHpwAoAIQxTLcvrKaOTrU+u22iCb8MrGZlvI9yC80uulZUxOpxmDdsvExv2npGnlwrcDWUy1jVh3PiGARk3u/e+ZcHFxjFk2XsAEPjcpPRy7nnLkk7NABAE/JrNGh2FizluwflgnaDb+FijW7NlwdHTGRgBb+qIKoQQ4nRDC5AQQohE0AIkhBAiEbQACSGESISGFqB169bhsssuQ2trK7q6uvChD30IO3bsmDKmXC5jzZo16OzsREtLC2644QYMDAyc0kkLIYQ4/WnIgtu8eTPWrFmDyy67DPV6Hf/9v/93vO9978PLL7+M5uZj3UBvv/12fOc738EjjzyCQqGAW265Bddffz1+9KMfNTazCK4Fx+URZEiXwiDFM5sqNe59WBZcTESWlNFtNGt0W80ZdlzeyHdrIsYbqwFAZFhGodFFMiZmlxdathv/+SRl5X4ZHWvTRGCzOmsG44bV1mi95FpMfpXP28q884z98cbdHWIdWAHAi4162uhCG/Bb0iNZcH6ZZ4rB6Hxq1QNigmWMDqcZ41qxOp9aWXDMXPWZLgkAaW6qIcvvcS9P3g9a+b3pj/N7OS4bhp3VFDTrvlGkfcOKbebnPh9ya64tzTste547viXP3yd8MhYAIqMjasjyHq0up6x+gh1RG1qAHn/88Sn//9BDD6Grqwvbt2/Hv/k3/wbDw8N44IEH8PDDD+Pqq68GADz44INYunQptm7diiuuuKKRlxNCCDGDOalnQMPDwwCAjo4OAMD27dtRq9WwatWqyTFLlizBggULsGXLFrqNSqWCkZGRKV9CCCFmPm95AYqiCLfddhuuvPJKXHDBBQCA/v5+ZDIZFIvFKWO7u7vR399Pt7Nu3ToUCoXJr/nz57/VKQkhhDiNeMsL0Jo1a/Diiy9i48aNJzWBtWvXYnh4ePKrr6/vpLYnhBDi9OAtRfHccsst+Pa3v42nnnoK8+bNm6z39PSgWq1iaGhoyqeggYEB9PT00G1ls1lksyQmow73QRZ/jobWpqJT6yjMpmMHJo7Set144JxKkZgfI1rHkgqac7whXWuex2awejbPH0RWSIM5ACgZsTj1qvugN2U9bDdiVzwS5wMAQZXX/TKJ/ynzB86WnJAyGtWljJgWVk8ZEkJg7I9v7E9MYoFiY38skcP3DQnBN8bThnRcQojHRnl9nEsIHrkmPCOGyHoKb/TuM1oXAhGREKKAywa+EU8EI4bKI3KCN240AJzgET2eFc2VNqJ4Wtz3hBR/SeSNg9Wc5vvTmuX3vk9Eq5YMHxsY11VsSAhxxOrGST4JCaGhT0BxHOOWW27Bo48+iu9///tYtGjRlO8vX74c6XQamzZtmqzt2LEDe/bswcqVKxt5KSGEEDOchj4BrVmzBg8//DC++c1vorW1dfK5TqFQQD6fR6FQwMc//nHccccd6OjoQFtbG2699VasXLlSBpwQQogpNLQAbdiwAQDw7ne/e0r9wQcfxH/+z/8ZAPClL30Jvu/jhhtuQKVSwerVq3HvvfeekskKIYSYOTS0AMXWL3p/iVwuh/Xr12P9+vVveVJCCCFmPsqCE0IIkQjTtyFdBNekYLk4AM5ccI5Tu/iCy+nYF/fuoPVBwxAKSFOpXNZqPMeNmqYct+Pa29ppvdhadOdhbDuKufFUM+y48oQb0ZOPuX2TMaJBfCOmxbbgXOMrmDAazxm2WzBumHclXk+R8SmeroKUkQDjG03ZQGJ+PMOC86lNZDf784xoJZ/EBYVlI25pnDeeiypGo7qYNLujI39FAzPDkLJ+wo0i9zthZDRwM/RXr2I07ysTC67C7wfUDTvOOAJxE4//iQ+792FQ4G+v+Qw/x4UCv8frxMQFeFO/nGHo+paNaUUl0UgoQ21rQJhzX18IIYRIAC1AQgghEkELkBBCiETQAiSEECIRtAAJIYRIhNPKgksF3BJZtuRip3bVivfQsWGOWyxvHjrAx3uu3ZQ2TJN6nZtdFSOzK00MOwBobys6NT/N512ucbWrUuGvOTwy6NQ8n+dH5Yy6ZwhFMPqGYcI1ijxSAwDfyHbzSoYdZ9TTpJ4xLLi0Ye8FVUPlIfsf17ghFBkNEJnVBgA+MdIAwIvIeMNqi6rcjkNsnTj3NS2JqW7YYZ5Z57B6YPw8HBhvUz64BReEJNtunN+bcWQ1BjRsxBZuzUXD7sXvD/MLLtfG3z8KRtO8mpGBGQXu9gOj8ZxlY6aMY97EsieN9yuzU+gJoE9AQgghEkELkBBCiETQAiSEECIRtAAJIYRIBC1AQgghEmH6WnAEy4aZ0z3HqZ11xtl07M5De2i9o30WrY9X3Vwt3+c2yNFh3m01LL/CX7O1g9Z7ZrndYzuauulYo0EjxkpDtH7w0H6nVvVa+bat1wyNvKmKkTdVdo0if8LISDOsNm/MyI4b4wZOmozPVPhrpo1su1Sdm1Bpkqvlw8jxMnSysGq0m60bnThD13gK60ZHVHD7yoNhfBGDLbKy0Iw0OLt+4nac36AFl7IsODIX30jyjy1zs8StU490wz02nhzzMT42bXRhbYn4fVUx8gFL7HwaYX0e6agLACnDU+wsuO9NmTzPtKx65CC+HR1RhRBCiFOFFiAhhBCJoAVICCFEImgBEkIIkQhagIQQQiTC9LXgfDgmhZWptn37Nqc259wz6NjSyDCtF5u4CZbPucZKzVBnDh3up/W9e96k9ZGhIVqPSKbc8otX0LFjRubb2Cg38gYHB8g2DtOxaOI2VZBzLT0AaK5zSyZdd1W9lCET+VanVGYZAUiN8nPBLLis8ZoZI8ctTbp2AkDGd+2rtM9vpQjcpqrFfD+rdSPbj+WbGRlclu3G7DAA8Imp5hljQ+M1rbplwTEnzerOGRi2W2hsm52JwOzmaZibVX6xeBVjPyfc8fE434Z17TdFfD9LsTH3OrEXfeOaCPmx4lcn0N3R5dSKhSIde9Aj7zWy4IQQQkxntAAJIYRIBC1AQgghEkELkBBCiESYvhJCAByfQBEbD243b/2BUyu38Cd9xV4eudMU8AeAHe1uJMVExB/87z+4j9aPjvCH/IcPcmlhbMwVJY4OH6JjZ3dxIaBkSAiVihstNDzIG5iVD4/QetTMG6G1NZ1B612RKydkTAmBP0T1S/yYB2OWhOA+RM9W+ZPRTGhEwBgSQsp3H+iyGgB4Rms3S3DwjQfOLBrHag7HGswBQGDMhdWtaJ2oQQkhNCN63Nf0zCgevj+RsT8ReQJuNbvzjbdAr84vUEtOiKru/kdG3JJvRD+lI6OZnCEQ1GvuXIxLmTc0BAy9AyjkW5xac443qKSnwepoeBz6BCSEECIRtAAJIYRIBC1AQgghEkELkBBCiETQAiSEECIRpq0F5/mAd5wGF3tcrXhz706nVv4xt6bOu3AZrZ+5aAmtd7e7kRRRjptKB4cO0vqePa/R+tFRbsHt6vu5U8s28dCMJYvPo/WaYfHAc82ccs014wBg+DC398rxEK13dPKXPKPJbQ6YqRqds4z4Em+Mm3reqGHHlVxzKAi5ZWTZboEV0+IRd8g3GswZNpAZlxPxeoqYYGlj475Zt0w991xExvysemiOt6J4mHl38rbbse245zky3up8q0mfcR58EpMFACAN3yLDXrPkRS/i+xPW+GuWSu59W/OMCCVjf6zEnImSe79VJ4zufWx/LEHzOPQJSAghRCJoARJCCJEIWoCEEEIkghYgIYQQiaAFSAghRCJMWwsu9uAoGn7A18sodnOYBoe4wTU2xvPNMmmeitRCMpHyHW107OKzzqX1Pft303ppfJTWw8hVSAbHBunYo6NHaD2d4qc2JHl6tZibZyMTQ7x+lB/b2WPcMrt4brs7tsyPdzTOTZvYsOAiYusAQFxhfo91uVs/h/E6yySMjRwvi8jMSDMsQDLebyBn7dg2LCPN3Y5ltVnZbtb+WAYbq1pjDV/SnCPbn1QDzfiObYSPD2JrvFsPY2PfjTq77wGgUuX3xPDokFMbD3lDw1KZ5zdax7Zvn9tE8+gho3ElOw2y4IQQQkxntAAJIYRIBC1AQgghEkELkBBCiETQAiSEECIRpq0Fd0yIiY8rWcFa7joa+NxKymRztJ5vdm03APBJp9TA6J66YN4iWr/owstovW7kMx097GbKBbksHVs1bJ2wzjWUcuhaVrWQz6MW8S6XExPcJNxf7aP1sTa3O6tXLtCxfokbP/VxbvHEhiEEkgdmd/nkP4d5RlKWR7ZtdfO0tmFlpMHoLMqy4yyDy85UO3GDzRprWXChOZcTxzoi1rZNg43WG8vNs340N2IDAZJTaUS7mbZfJeTnvlTleYfjFbc+XOZm7SGjK/O+kQO0/tKuF5zaxKDbqRkAV+lkwQkhhJjOaAESQgiRCFqAhBBCJIIWICGEEInQkISwYcMGbNiwAW+88QYA4Pzzz8fnPvc5XHvttQCAcrmMT3/609i4cSMqlQpWr16Ne++9F93d3Y3PLIITxRMbURVsHU1n+EP7pib+8DvXxCWEyHefJFZq/GFha4Fve9n5F9O6tfz//NWXnFpoNMIKyfwAoGo0pCvX3KiOSp3vT82IHQl9IzIk4jEg1QqJyylzGSSa4NE6YZk/iGUxTAAQElGgbjxAt0SB2DhBrBFaaIz1zZZfHKvhG887saJ4GpMQmFgQmk3grLicE4/cOYZ7XGwJwXpN61+cuPpgnp2Af8dPG2JK2j3/cYqPtZrGjdX5NT5WNe4Jct6MHooYNMShnf27eH2P2xQTVaOhITncRtqQQ0OfgObNm4e7774b27dvx7Zt23D11Vfjuuuuw0svHXvDvP322/HYY4/hkUcewebNm7F//35cf/31jbyEEEKI3xIa+gT0wQ9+cMr//8Vf/AU2bNiArVu3Yt68eXjggQfw8MMP4+qrrwYAPPjgg1i6dCm2bt2KK6644tTNWgghxGnPW34GFIYhNm7ciFKphJUrV2L79u2o1WpYtWrV5JglS5ZgwYIF2LJli7mdSqWCkZGRKV9CCCFmPg0vQC+88AJaWlqQzWbxyU9+Eo8++ijOO+889Pf3I5PJoFgsThnf3d2N/v5+c3vr1q1DoVCY/Jo/f37DOyGEEOL0o+EF6Nxzz8Xzzz+Pp59+GjfffDNuuukmvPzyy295AmvXrsXw8PDkV18f/2t6IYQQM4uGo3gymQzOPvtsAMDy5cvxzDPP4K/+6q/wkY98BNVqFUNDQ1M+BQ0MDKCnp8fcXjabRTZLjDXascqI08hknNo8IxZnwfyzaL2ptUjrMWnsZnlK8Hj8T3txFq2fdeY5tF4hBtthEs8D2FZStWKYbSR2pxYZdpRhwXlpI+bI4xFFHospqXFLLyrzaJ2wZjSqM0woZgjVGvx5i9luAOCT7VgmnWXBBeZcLH3I3U/PNM8ai8thsUDWdfX2WnD8WFlN0+IGDENrHuZ5M65xP8vfMr2cWw+z/BxP+PwdZKjO93S0xu04EAM2w95LAYTG5TZSGaP1CRb/YyWh8fIJuYgn/XdAURShUqlg+fLlSKfT2LRp0+T3duzYgT179mDlypUn+zJCCCFmGA19Alq7di2uvfZaLFiwAKOjo3j44Yfx5JNP4oknnkChUMDHP/5x3HHHHejo6EBbWxtuvfVWrFy5UgacEEIIh4YWoIMHD+I//af/hAMHDqBQKGDZsmV44okn8N73vhcA8KUvfQm+7+OGG26Y8oeoQgghxPE0tAA98MADv/L7uVwO69evx/r1609qUkIIIWY+yoITQgiRCNO3IV0IV6/gYgoWLTjbqb3vd/4dHXvusuW0Xs3wjVdIblOQ4YctJo3xACA0Gs9ZeXUdHZ3u/Ixst9FR3iSqarxmjdRDI2PPStoK0nz/m7PNtJ4OiCFkWHB1Yz+j2MjCMxwp5gAGZuYbxxrPzKlGLbi0cTHbdtyJN1mzTTXLGGzEguNYppplx7Gj0kj23q+qc8POaBiYMmy3HDc6/aYTr1cNC24s4NfysHGNs/cgAPDIfWhdP2nDjmvvmE3rs7t63aKxWsTkNLAaQ5+AhBBCJIIWICGEEImgBUgIIUQiaAESQgiRCFqAhBBCJML0teBi4vgYWWuXLXOTFv7dNf+ejk13dND6zwf20HqNdCnM5Xk3T8sOq1SNTqFGl9Mg5Ro1uRx/zbESt+DqkdEplOS+RYYF5wf855OWZt49tquti9YzvpvVVzXyrezMN36s6tR3A6rUvrLsMCuvjcPNtsYy32Lj1kvDyNMjNctUixu249hYyzxrzFSz7Di+R9Y2rNfkZ4iN94xtBFn32gQAv8W4x1t5PW5xt1PO8/0ZSfFreTwwjpaRPxdE7muGJOsRAHKZJlo/o/dMWj/77PPcYhM36eJB/v52IugTkBBCiETQAiSEECIRtAAJIYRIBC1AQgghEkELkBBCiESYvhYcg9hhALBsyUVO7VxmcQAYqHPLyj96gNbTgWsO5Zp45hkCI2vLkETC2Ojw6rs/FwRGZlUYc3OmXud2WEQsOMuaShnHu725SOvdHYYFN+pup1bl9l5Y4/P2DAuualhwPvGvLAusbtStLLhGLLiUaXBx+8qaI+vCamW7Wftj1dkVZHcb5dehdQ1ZWX18+0aWomm7GZmMZLyX5tdyqiVP636BW2NhkY+PWt3tl5r4MRzLGPmFRnZcqombd2mSmxgaxm024AZbwS/S+pzuhe48WvgxqafIa1qhgcehT0BCCCESQQuQEEKIRNACJIQQIhG0AAkhhEiE6Ssh+HCe62az/AHgnN55Ts0znqFGxoP/VIY/FM4H7sPFdJY/FKxHVoyMJRvwh6vptDsXPzAexBqN58LwxCUE3zceoBsPbtua2ox6K637JfJzjtEFznr0HRtPNWuGhAAiLVgPxK2bwBIIeEM6TmA8QLee0ZpNAMl2rEZ6/Iqw60YQkzkTPtoaz485a1TH5IFjY/kZqhn1yHOv26CJv3d4RR4rFbUbolGR3/u1gvua4838qpgwInriZv4elDbqGRLlFRvvE9m0sf/GeUsF7mtajTjrJ/ExRp+AhBBCJIIWICGEEImgBUgIIUQiaAESQgiRCFqAhBBCJML0teA8OGpRYBgeecNwYQSG8ZXP8W3USLyOZaTFhsLkGUpeKsUPf4ZYdhlixgG2CVWzLDgySd+YX9qI4sln+LFK+0Y0SujOMhXwsak0j/uolEdpvWY4XBHceJC6YWSlG4zRYc4bM+OObcOan9V8zdoOmwUfy5vxwfQF2Vx8sx0fr3tmIz0risc9LjXjeNeNt6mq8ZpoItdQkZubtU5ubsYd/DqM27kFV20jFlwLPz/1ZqM5XjPfthWBkyeHNpVx43kAINvMrT7LjBwbd+83K97rZNAnICGEEImgBUgIIUQiaAESQgiRCFqAhBBCJIIWICGEEIkwfS04kgVXq3HD4+jw0RPebNqwr3JZ3rCJeSKxx82mRjPfMmn+mjHJa2ttKdCxLc3c7jlkGGkgWXisAd6x+Rn5eBlu6/ie0XyN7E/aN3Lmcnw/x0slWh+Jxmi9SrPgDDPQtK8s46uRhnRWozar+Rqvp0ndM+ZnmU1VY46sOZxlwfmGeRabP8taFqBbt/a9btiYMBpDBu3tbrGng297Nr9/wllWQzp+z1Za3fut1mJcP61GBmQLz6XLtvD9bCVv37Uqv8Yzhkk3TmxRABg42u/UwgrPujwZ9AlICCFEImgBEkIIkQhagIQQQiSCFiAhhBCJoAVICCFEIpxeFlydWxg7Xn/Fqb1rbJiO9QyjJGuYNjWSnVaPeb6Vlc2VNjLf0ilumUWky2mhjZg9AOZ0u91gAWDoyCFar45OOLXQM8wZY35NRhaclZ0Wkc6NVifGtqYirbcaMVTx0DitD8K142rg109kdkq1kvbY3BvLdrO6s4aGw8aMvMC09/hrWllrzIKzzo/V+TQ23koss81Lu+P9LN+Gn+PXIdq4NeZ1ucZb3G1YcF18G7VOw4JrNyw4kgUXtfL3lHQrt0izhgWXM+p+4G6nWjW6MhuS7+Eyf5/YM/C6W6zxm5Bdbdadczz6BCSEECIRtAAJIYRIBC1AQgghEkELkBBCiESYvhJCAPJ0iz8Ee2HHz5zarjd20LHzFi+l9ZTRlM2P3MdpsdF5rtHGbla9XncfOGfS/MFld2cvrY/NHaL1yoj70L5U4cJGznhy2WxE8WRC4+eZ0H3gHpLjCgDZLH/gWujkkSmpgG+nftR9GDsU89iREDziyTNiZNiDePYgHwAi43Esi6I5Nhder5N6YG7baIRmxOvwGJ3GGtLFVmwRaa4IAH6rKwMFzcYD/hy/rqrGw/w0idfxe3jEUzSLX2/1TqNBpRHFE7W59VSb0bixjQsOuRbeHC9nNJNLp937KipxueVo6QitDw67kTsAsKuPvH/WDfmKXYbxiYkI+gQkhBAiEbQACSGESAQtQEIIIRJBC5AQQohE0AIkhBAiEU7Kgrv77ruxdu1afOpTn8I999wDACiXy/j0pz+NjRs3olKpYPXq1bj33nvR3d3d2MY94Pj+ZrFhPB04tNep7d77Gh1b6OXWGEg0CAB4xNbyGrTgUkZzuJTRHM9jjd34S6Ipy42anllzaH14lmvDHOZpNmiqcUsvZ9h7qSrff9TdyceGBRd7/Jhkm7gF15224mhcC656hO/oGHhDQ+OQ0wgcqzmcFcVjJAsZAT1AnThFnukZ8blYdhwz2CyrLbQsuIBfExnD4Mq0u1Za2Mojd0YDHi9TbuHnvjDLfc2WuTyKB4YFV20zooWM14xb3P3PtnILLttqRO4Yx6qpyTD1Um5s08g4t93ePLCb1l997ae0vqdvl1s0Ljfjrn97LbhnnnkGf/M3f4Nly5ZNqd9+++147LHH8Mgjj2Dz5s3Yv38/rr/++rf6MkIIIWYob2kBGhsbw4033oj7778f7b/U/nZ4eBgPPPAA/vIv/xJXX301li9fjgcffBA//vGPsXXr1lM2aSGEEKc/b2kBWrNmDd7//vdj1apVU+rbt29HrVabUl+yZAkWLFiALVu20G1VKhWMjIxM+RJCCDHzafgZ0MaNG/Hss8/imWeecb7X39+PTCaDYrE4pd7d3Y3+fv4Xt+vWrcOf/dmfNToNIYQQpzkNfQLq6+vDpz71KXz9619HLsdjMBpl7dq1GB4envzq6+s7JdsVQggxvWnoE9D27dtx8OBBXHLJJZO1MAzx1FNP4a//+q/xxBNPoFqtYmhoaMqnoIGBAfT09NBtZrNZZLMkX8kD4uP1ijT3LQJiQtVqPN+rVjOakvnGgkqazxmyG3zDBwmY1far6uTngtA5GMeIjJ8hWvLcnOkszHJqtVa3SR0A5Cw7zue2UiYy9j9kJiHftucbPxMZxmDK5/lZs9sXOLXB0YN0bKXK99/OiHNNMM+4layMOLtpXGOZcgyrmRxg2Itp19byM0YHM+P8RIZFGncU+Xa63AaL1bxhuU7wrMJRj9/LZxfd89O1oIvPg+TGAUA9zT3FkmdcK2lyjee4MZhr5nZck2UMpvm5iD3X0xyvlOjY1/fvpPVXDAtueIjYdMb7HntrMt6uHBpagK655hq88MILU2q///u/jyVLluCP//iPMX/+fKTTaWzatAk33HADAGDHjh3Ys2cPVq5c2chLCSGEmOE0tAC1trbiggsumFJrbm5GZ2fnZP3jH/847rjjDnR0dKCtrQ233norVq5ciSuuuOLUzVoIIcRpzylvx/ClL30Jvu/jhhtumPKHqEIIIcQvc9IL0JNPPjnl/3O5HNavX4/169ef7KaFEELMYJQFJ4QQIhGmb0dUD4514aW5xTO70zVciq28A2LgGxlXkZHCRXLfgoArHp6hx1kZXJYFlyJzDI2xlm6S8vh+suy4lhy3b7Ikww0Acsa208R2AwCfZcGRLqkAEHlG3UhJC2J+XPIp127qbOIm5kiVZ8GNY5TWG7Hg7Fw2fqyqxrXCfLzY2IZvnJ/mZn5PZNvcnLRUE88YrPj8NStGA1W/s8hfc85spzaS5trl7t0v0XrfMD9vftXNnVzQsZyOLczrpPXR6hCvH+Um5dCg+wf0beBGWrqJW6SFoEjrgfW+ErlZcIeHD9Gxr+99ldb3GxlxNWbTNRL6doLSpj4BCSGESAQtQEIIIRJBC5AQQohE0AIkhBAiEbQACSGESITpa8HFcEyKbIbntc3rnufUZre7lg1g22ETdZ79FBP7yup8aqkfRiNXmvkGABnftf1in8+vZthhvmHH5VKugdNsHNdsmm87F/JjmDKsOdRdW2eizI2n4Trv6NhSNzLvUnzuXs096PkUN7vy4Ll5NaNvKZd++K1kZfXVjGtlwqy7hMbYpmZ+TArd/J7IEYvUy/Nt1I3rzfPccwwAvpEF1zTf7djbnOXXxOAA3/YrR96g9XCve54XDF1ARgJLzuQWXNnjc9l39E1af22/a5Nls/wYjlfGaL3ZyIJrz/LzNkFMtYNHeNeBAwNu12gAGDrKrblapeLUrPxG2jrYaid8HPoEJIQQIhG0AAkhhEgELUBCCCESQQuQEEKIRJi+EgJ5iNWS5Q+L53fNd2qdLW68CAD4RtO0uMYfdHqk6ZPV8MszntKRNB8AQNrYTkREidg3Gn4ZD39twcHdTpPR8Cob8IfwmZohYRjHsFpxG4cNjw/RsX0jvPlYlOGxOF05t7EZAGSJKFIZ5w3MAvBolDS4tFAjF2doRO5YDebKxlPaMeN8jsCtRxkjEqrAH37XZvPmffVZbmyRZ3Q8rseGDBPx5n2pIm++lu1y78+erl46tvOQKxkBwMiBp2n95UNu7Mz/feVHdGy5SMsYD/h52HfwdVrvO7DLqVkRXOkMv5fnzpnLxzfxCLJS2b0nUgG/DgutRVo/YLyvVCvu+Yz5IeFygqJ4hBBCTGe0AAkhhEgELUBCCCESQQuQEEKIRNACJIQQIhGmtwV3nElRyLm2DgAs6HBjPdqz3PgZqxlRIlWj4RlRPKyYG6M3mmmERKHRaCp2TZbIbGxmxOIYk2HmXS7gFljWkN2CiO9QWOOGVBS6sR5RyJt1VcvcVIvKfPzEGLfmmkicUb3OAm2Autlpy4j5IYad1SzRS/PzUzeilUYiHtNymChIUZ6fh6iV1/P8lkBQcN8GrKZpFeO6qhj9HFOtfDsouIZhz5kL6NBLalfQ+vf3crNt/9H9Tu3Z17bTsdVmPvFMGzcgDx3dR+uVqnveYqP5ZdmI4ilNuE3tAGDMqNfqrqm2oHshHZu7fBWtNxuG7paD33Nqo9EgH6woHiGEEKcbWoCEEEIkghYgIYQQiaAFSAghRCJoARJCCJEI09eCC4Hj5aSOTJEOndfW49QKnpGHZRglmYrRUIss0YFhAsXG0fQNCy42zKGY5NXFxIwDgMiYS8ow9djcrUy6lGdk1dX5xCeqru0GAHHdNdsij2eH1Yz8uVRo5exx3aYpck+GZ6g5daNeMwxDL+vmmwVtXDELWrgdVzcanh2e4KbeAdLUr5bhx2QoGKL1iegwrZcD1y5tN5raRYbZVSXHGwDSLXx82OSOL3bNomNXtl1F65f1cQvu//z4207t0BhvvPba/p203lou0PqoYbDVI/fazxkmbnMLbzyXyXJjsB4Z+XvEOu1s4c3rzp17Nq0vKHbT+tg+13j78ZuuGQdYDRpPDH0CEkIIkQhagIQQQiSCFiAhhBCJoAVICCFEImgBEkIIkQjT14KrwbHgZuc66dC5uS6n1lblRklp3LCmjDCrMHBtssCy2rjwBO6GAaTZ6rE6yYiLrdw4o8NrKrLsOLceGLaXb2SkVSOje2zMzbbIJzlmKcNIy/C6XzVe0zCEqqQNrWfsj5XhFxv5bqlWNycsO5t3ZvU7eKZYBTxX6+jgQVo/RHazZhzDo0b32MHKXlovld175cyAW3D5Jt6V2MokzDZbFpx70L0cP96L5y6m9dXv/re0/vKBV5zanoE+Ona0zK22eIJfKxPE6ASAGrkO2zL8Pai9UKT15hZuzXkBv0DroXtPZMCP4ew8t+PmLn0Xre+8/CWn9uMnn6RjY49cnFa84nHoE5AQQohE0AIkhBAiEbQACSGESAQtQEIIIRJh+koIJIpnbksvHdqbcR+wNY/zh5/5Ef7gtqXMJYQoRR7a1/g2IqP3Vso3HqwbgRXspwJLQgiNutWQLiBbD6zIHWN+ZY8LAVHa2B8SRxO38YMVGzE/npVnZIgcFVL3SSM5AIDxkNfLZ2k9XXSjeKJOHq9Sm83rIxGP4hk0HuaP1txbNcoY8ybiDABM+LypX1jrd2qpOo+i6TIa7KUC/laSyhqxTSn3GirV+DFpC7jIcdXyd9L69t3PO7XHNn+HjuVXMjBR4+pQuc7rrEdjSzOXCtqLXKZqauLXShxYMVzuRV6pcEkinOB72p7voPULz7vUqeXa+TVRPkgintSQTgghxHRGC5AQQohE0AIkhBAiEbQACSGESAQtQEIIIRJh+lpwERwLbk6r23gOADr8olNLjXBrKjPIbZD8BI90YX3g/LzRHC1nmGcpy+wyOtIRay40jLQq028AGC+JgDSq81jXPQB1n9tUlRR/zSjPx6eLbqxLqu42QQOAOMWjRGolvkPluhVTQuZidclK8dsgaHZtNwDIznJtpWg2N7UmOrjtN2zMezzi+1+tEiMvzbfhpwxryvh5cyTrXvtHfB5Rkwm4qZYOjHnXuQ5VLA04td6JuXRsa8SP7bzCHFp/92Xvdmo7D+yhYw8O8+gjFh8FALUy35/Ac495e4E32Oso8nqGNDoEgAr4tV+tu9FXEyUehzU6MkzrvmHBLZy7yKnNnuNGngFA325ZcEIIIU4ztAAJIYRIBC1AQgghEkELkBBCiETQAiSEECIRGrLg/vRP/xR/9md/NqV27rnn4uc//zkAoFwu49Of/jQ2btyISqWC1atX495770V3d3fjMyMWRSHDzanUBLF+ahN87BFuiWRKPEMpIgaKl+Vj4ybDPuKRYjD6byHKuLpWzdDaAsOQ8Qz7yCcWnG9YcLFhWVUz3HarGRZcps29zPIeb2yWzvFGaNE4t5LKRoZfWHbNrtAwBoM0N9WyrdxKiki+W20Wn/doGz8m4yF/TT8wrvGKO/e6Z+QXGpF3VkO+kJznCSPDrZTm909g5AMeHR+i9fI+9/y0tBvZaT1FWm9r4Xbc3FnzndqSM5bybQzxRoJ1n1uxg2O8kWA9dM9P9yzD6ms1mhcamW9hjc+lXHXfh0ZHhujYoZajfNvdru0GAE0t7jXeapwf2gPPsHCPp+FPQOeffz4OHDgw+fXDH/5w8nu33347HnvsMTzyyCPYvHkz9u/fj+uvv77RlxBCCPFbQMN/B5RKpdDT4/49zvDwMB544AE8/PDDuPrqqwEADz74IJYuXYqtW7fiiiuuoNurVCqoVP41YXZkZKTRKQkhhDgNafgT0M6dOzFnzhyceeaZuPHGG7Fnz7E/8Nq+fTtqtRpWrVo1OXbJkiVYsGABtmzZYm5v3bp1KBQKk1/z57sfn4UQQsw8GlqAVqxYgYceegiPP/44NmzYgN27d+Nd73oXRkdH0d/fj0wmg2KxOOXfdHd3o7/f7TfyC9auXYvh4eHJr76+vre0I0IIIU4vGvoV3LXXXjv538uWLcOKFSuwcOFC/NM//RPyef7A9teRzWaRzRpP6YUQQsxYTioLrlgs4pxzzsGuXbvw3ve+F9VqFUNDQ1M+BQ0MDNBnRm+FuGL0LxwlZs4EN9WCQd7RMBjmGVdh1a3XrSy0JqOzaBM/zF4LX3hTxKYLjPw5LzB0k8iok814Prdv/IyRy5bjVk7d2P9q3bWvIiM7LJfndd+w4Orj/DxPjLnXRERMJQDI5Pj+t7UbP1R1ufaV38mttrEmIxTL490vW5vc7r4AEFbca8XqIFoxDNDYsABj0uW0ljW64ab4ufcNw240GqX1I4dcK8t7hW+juciP1Rk4g9YHB11TLcVCHQF0tvAstGwrtxpndXCjt0o6+XZ38g7O+Qy396zzUyOZbwBQIV1bS2We4Tdq1Mvg2x6aGHJfL+LvnUYT3xPipP4OaGxsDK+99hp6e3uxfPlypNNpbNq0afL7O3bswJ49e7By5cqTeRkhhBAzkIY+Af23//bf8MEPfhALFy7E/v37ceeddyIIAnzsYx9DoVDAxz/+cdxxxx3o6OhAW1sbbr31VqxcudI04IQQQvz20tACtHfvXnzsYx/DkSNHMHv2bFx11VXYunUrZs8+9muDL33pS/B9HzfccMOUP0QVQgghjqehBWjjxo2/8vu5XA7r16/H+vXrT2pSQgghZj7KghNCCJEI07cjKmHw8CH+jSFieJS4CeQfNQyhIzyBoTrhWjw1j5sjdSMLLW7ltpvXzvPQ4qJryXiRkdeW4VZSBG6NsTywVJqbZ1Gez7seG/lroTUXYoLl+LGq13g9ynFDaNwwEsdZt1lDSGs27L1gNt//YLZrx3lFoyNo1uig6RsWHL8kEJOOqNE4zyWrlfhrRqFhkZIOqiHJIwSAitEp1IifQ9Ww5kqhe1+9enAXHdv6Cj8oR6tDtH5gzO1yWjFsyVSGK1x5n1twmbyRVUh+li80FenYwONvu/UaP7bVCrfPwjoZ7xkmnfF+MAZuUr524HWndnTkCB1LP8ZY3YdP4J8KIYQQbztagIQQQiSCFiAhhBCJoAVICCFEIpxWEsKeN90HYwBQOzrk1NIlLgrgKI+kqB0a5vUJV06oBlYUjfEAvc4fXAaBIRZk3QejUYafqhDGg3/fevjtbieT45EzcWA0njOe5vuGnIC0+5q+IVWENUMqGOWvOerzOZbIs2U/5q/pt/IYndosflzqne75tGKV4pRxrDyjeZ/RTS5PniGPe/zhtB/yB8ux4SB4REKIDLmjajRqs2JkqkwGARCTw1UxYmH2jxhhxgf4+SyxKKKQn4dUzOURj8RHAUCKXMvH6u41kU/xyB3POMf1Cj+2FSNWLA7dY5tLG+JMls97tM6jkvYcetOpjY1zUYu8pSD2TsxD0CcgIYQQiaAFSAghRCJoARJCCJEIWoCEEEIkghYgIYQQiXBaWXCvvfEqrQ+87kZ4zPPb6NjwMLfdJg7xWJOJqmvNWbEwIYxIm4zRqK7CrZ+YRHKEoWHOGHZPaERyBCl3jtkUt8BCw4JDzG0dz7DgUll3+2mfH6vY2J84zbddQYnXyfYtC65mWHBRkduLKJAoniY+1g8MC84woXzS2AwAfN+dY2B0EvYMUys2zhsTEuuGRVm1rEujAaI1Pk67+++n+Lwtk26IxGQBQC0ir+kZBqRxTXjG/ZYy5phPuddE1uPXlZGKg2rI3w9qZV5nJmlTlpt36TSfi9XUcGTcfZ8MfH6ssuTaj+sxSjAa2P0S+gQkhBAiEbQACSGESAQtQEIIIRJBC5AQQohE0AIkhBAiEU4rC254/CitH96/16nNa55Hx5b6D9P60CG3iRUAVEj2lQ9ulHhGQ7ooMjqhGXUvduuGlANDpkJsWD+ZjGus+EbjubBi/HxSt7KpuPEUeG7WWGBYOZFhwUURbyRYr/CGYvXQrZsZXHnDUmzmtwfLfUs3c/sotmwyfqgAy3git2oqw8+bH/B510IrncvVsibqxgSt/EKSSwYAVcO8SxHD0jPyDqMUP28TkdEYkjRqCw1LL7COiXHDpQ3TNUca2FljYyPvMDTC+qzxGc/dfsZoLmnZe5U6N9VikvfYlOPXeIpkYEay4IQQQkxntAAJIYRIBC1AQgghEkELkBBCiETQAiSEECIRTisLLjI6cZZKbl7baMi79/Uf5N0V9x/hdY+YUE3N7XRsxjCEopRhDnGBCx4xhCIj3wsZI/PN2HY+3+rUWooFOjZd5dbLWJ3bR5WQWzw048uwdSLDpKtPGAZb1qq7Nc8wnmrGMWSdaQEg3ewaT9kWo/ulx6/ZcpkfK69uvKbnvmZnehYd6xsG5EB8gNZLE+79M1bmpmPNsMmYuQnYdpwfuCfIy/B9J0IjAKBiWGM1lqlWNzqiWh1brea+lgUXuOcn5Rn3rLFtq6tsYLxR5AI3fy6X4XZpKuDzDg3r1CPXUJZ0fQUAn+Q0Wu/Vzr89oVFCCCHEKUYLkBBCiETQAiSEECIRtAAJIYRIBC1AQgghEuG0suAyxJwBgGzBtbiqxq4drvNMsf2kAyAAZDOu3TQrxS045IxOlHkjE8qwrPws2U6O73sqz7eRMroXFtqKTm1u7wI6thJxyyhl5LjtHxygdZZxFRoq0PAEPw9VI1OtHhi5WuSQe0aX2NDYRmCcz1yLax81tTbTsSlDpxqPuWUWGNl2+ZxrL/Z0dtOxacN4GjjMz8/e/j6ndniEZyaOGudnouyadIBtx+WJ8eYbBhcsO86wxmrkmFvzsMgYeXrNGX6em7NuPWe8X8UwTEfjPStPDDsAyJD7MGe8T1gGW824DkNiDXqx0cXXI3VWY//2hEYJIYQQpxgtQEIIIRJBC5AQQohE0AIkhBAiEU4rCSHX0kbrXUvOdmpNIY9GaS8fovVi3mjMxJ7ddfJ51Avuw2kAiEkDMwCAUU+TB9rZDi4+1LM8umZ04git57Lua87t6qVjUyn+MLutqYXWe0f5dsZJ06uDJf6Qe2w/l0QqRmOzmiEzxEwsiPlD6zqMxmYwGuyl3Z/bmpr5uU8bkSTjYYnWy1V+TRSaXNFm6RlL6Ng5HcZ5GB+n9f4jrpywf5DH9uzuf4PWd+19ldaPDvFGjykiHKQt0SbL5YS4bpz7qnueY98QTdJcCGgxrvHOYietFwtuPcjyB/9148d+P+Jvx6nIkJhIZFc6Z0QFZfj1WTca0tUq7j0RGcc7ReQES1g4Hn0CEkIIkQhagIQQQiSCFiAhhBCJoAVICCFEImgBEkIIkQinlQVXNoynYIFriOXzPKbkgm5ugxT2zaX1I+ODTm0ibTRk4+IdojZu8WQ6eSO4jjnu3NsXzaNj/bo7PwD46fOv0PrI6JBT653FG5stmrOI1hd0dNF6r1GfICbYa0Nu/AsAvHl0D61XDFunHvFzARATymgON17lRlr/kX203trqWpDZPD/H+RZuU+WNmJ/mCt9Oc9a1m1oNy6qY5nExswt8LvMK7nmbiLlht2+EW23PvMrP/U9e2kLrdWLkpTP8mFj1mmEYRp77PlFnTeoAlKvcDIyMZnfNeX7MZ7d3OLV0nr/XlI1tTxhzrFod7Eijy8A6hizeC8D4GG/cWa2Q+81o6OiDGW+y4IQQQkxjtAAJIYRIBC1AQgghEkELkBBCiERoeAHat28ffu/3fg+dnZ3I5/N4xzvegW3btk1+P45jfO5zn0Nvby/y+TxWrVqFnTt3ntJJCyGEOP1pyIIbHBzElVdeife85z347ne/i9mzZ2Pnzp1ob/9XC+0LX/gCvvzlL+NrX/saFi1ahM9+9rNYvXo1Xn75ZeRy3CA5Ud4Y7qf1v//BN53ah9/9ATr2zLO5TXb2PG4I5Y7ud2p7RtwaAIzH3KYKm3jeVHM71+ZmzXGNop6FPN9rYD9vEPbTHdtofferu5zajpd/SsdevfI9tH7FZe+k9UJhNq3H5OecnJHBVTWMtEp5lNbDGm+ohYiYQ6wGYLzCTajdJf6ao6OueXh4kF8T3d1zaD00jLyhYW41HozcYxiO8SZwEwu5wbao9wxab8m5Vl/G45li6QK3RQ/28mZ321/ZSuvDpAFklpiLAIDQyAGscjNyePioUxsb5PdJP8mNA4DBg9z2K1vHfNy9VgrEjAOAmnHuyxHfT98w21oKRbfWwjMjg7SRJ1cz7EBy7XsRn3dAjDfvBC24hhag//E//gfmz5+PBx98cLK2aNG/qrpxHOOee+7Bn/zJn+C6664DAPzd3/0duru78Y1vfAMf/ehHG3k5IYQQM5iGfgX3rW99C5deeik+/OEPo6urCxdffDHuv//+ye/v3r0b/f39WLVq1WStUChgxYoV2LKF/z1ApVLByMjIlC8hhBAzn4YWoNdffx0bNmzA4sWL8cQTT+Dmm2/GH/7hH+JrX/saAKC//9ivyLq7p/4hZXd39+T3jmfdunUoFAqTX/Pnz38r+yGEEOI0o6EFKIoiXHLJJfj85z+Piy++GJ/4xCfwB3/wB7jvvvve8gTWrl2L4eHhya++Pv4X8kIIIWYWDS1Avb29OO+886bUli5dij17jsWn9PT0AAAGBqY+kBwYGJj83vFks1m0tbVN+RJCCDHzaUhCuPLKK7Fjx44ptVdffRULFy4EcExI6OnpwaZNm3DRRRcBAEZGRvD000/j5ptvbmxmGThxQkMVbrJ85Zt/69S2vvwM34dly2n9rDP4r/7KcC2rfWPc+BmOuMEVZbnx1VnmXUFRdG3B7Hy+ML/8KjfYXnyZW3A7X3zRqe148Tk6dutPnqL1Sy++nNbPW7qM1jt6XIPvYN01lQBg58/5/gwd5B06I8Pi8SKSBRdyC67Ccq8AlEr8fO7bt9up7dj5Mzq2SEwlAAhIjtex1+SWVXXc3c9swLPGeo2OqIsXnkvrS84636ktO/9iOratm+cGvrqLn7dXXnmW1ocPufdQa5ZboU1GN88w5Jlq46OuwVUa5s+WK6O8A28Q8nv22e3c6pvT5dqBhWKRjkXAz31ofBxobm2l9Xnzz3Bq5yxeSsd29fBszL0Dr9P64EH3kUlQ58ZgxneXkdDnxtzxNLQA3X777XjnO9+Jz3/+8/iP//E/4ic/+Qm++tWv4qtf/SoAwPM83HbbbfjzP/9zLF68eFLDnjNnDj70oQ818lJCCCFmOA0tQJdddhkeffRRrF27FnfddRcWLVqEe+65BzfeeOPkmD/6oz9CqVTCJz7xCQwNDeGqq67C448/ftJ/AySEEGJm0XA7hg984AP4wAf4H3kCxz4F3XXXXbjrrrtOamJCCCFmNsqCE0IIkQheHMdG/kUyjIyMoFAowGsGvOMkhIg/KwZ4ggWl1XpwO7uTj293m3vFWb5tL8fXcz/HYzDypLEZAPT2uEJE5wJuEb7wOn/4vfXZH9F6bdyIrmEYP574pDkaABTa+TFs63QjSSLedw3lkMfiZFL8w3ouY2yIXNaRISFUDZFhnDRNA4DSuCsK1Kp8G77PI0msoBJrjlGNPNTlz+BZL75jc0nxC7er05UWlp77Djq23ZAQXn6TN0DcscuVXgAgrrs3bTrg5zjwjAsxNBrS1d1jGFX4m0RY4cc7tt5TjGPuHf9mBcAPuMgQe/wERUY9nTXOW5d73paccx4ZCZx15tm0PmHcby/vet6pWSJQMXRjzMJaiOe/8yqGh4d/pdmsT0BCCCESQQuQEEKIRNACJIQQIhG0AAkhhEgELUBCCCESoeG/A/pNUSy2OgbRyBCPKYlTroESGjbRWMyjN94c2UvrLXCtuaYWbtJlY/7HtmmPm1oTxlyGxt2mZNU3eFzOoaM8ZTwa4bYb89c8IxYmirllVK9xRWjwCJ/L4Igbu+IbDenyeX4MUy08jiTMGFoSEYrqdT7W2p/YiOjxiTnl1w0jy2jiZVlwhgiFgFzO1lgrBCUK+f4cHNjj1MoTPLomk+dG1ugYj8mKJ7hlFfjuNRd5/PyEJFbp2D8w9pQdK/52AN8yCY3xxi1BrcvYiAqKjZNvyn7G+8TooHtf7d1tNHo0zo9P3jsBICIRRYWA34MdqaJTq5sH9rjXP6FRQgghxClGC5AQQohE0AIkhBAiEbQACSGESIRpJyH8IhkojmLnYaqVGsTKZsCQUbfGR+QBKKsBQGREg4TGA2r4RgwIeQIaGmOj0DgmxkvS0dY2rCfljeKTB7SkBgCx0XOERtEAiLwT39HIEgWMujUXVo8bPA8m1nXLTr91LTf8kmR/rPPQ4LEyH+azG8643mJTQuBl+prGPBqtN/S+Yu2PdV9Z27a2Q455WOMTr1e5FOBHfONsO54hdtWJPFL/l3//65Lepl0W3N69ezF/Pm8OJ4QQ4vShr68P8+bNM78/7RagKIqwf/9+tLa2YnR0FPPnz0dfX9+MbtU9MjKi/Zwh/DbsI6D9nGmc6v2M4xijo6OYM2cOfKLc/4Jp9ys43/cnV8xfJMy2tbXN6JP/C7SfM4ffhn0EtJ8zjVO5n4VC4deOkYQghBAiEbQACSGESIRpvQBls1nceeedyBoNmWYK2s+Zw2/DPgLaz5lGUvs57SQEIYQQvx1M609AQgghZi5agIQQQiSCFiAhhBCJoAVICCFEImgBEkIIkQjTegFav349zjjjDORyOaxYsQI/+clPkp7SSfHUU0/hgx/8IObMmQPP8/CNb3xjyvfjOMbnPvc59Pb2Ip/PY9WqVdi5c2cyk32LrFu3DpdddhlaW1vR1dWFD33oQ9ixY8eUMeVyGWvWrEFnZydaWlpwww03YGDA7e44ndmwYQOWLVs2+ZfjK1euxHe/+93J78+EfTyeu+++G57n4bbbbpuszYT9/NM//VN4njfla8mSJZPfnwn7+Av27duH3/u930NnZyfy+Tze8Y53YNu2bZPf/02/B03bBegf//Efcccdd+DOO+/Es88+iwsvvBCrV6/GwYMHk57aW6ZUKuHCCy/E+vXr6fe/8IUv4Mtf/jLuu+8+PP3002hubsbq1atRLvP22tORzZs3Y82aNdi6dSu+973voVar4X3vex9KpdLkmNtvvx2PPfYYHnnkEWzevBn79+/H9ddfn+CsG2fevHm4++67sX37dmzbtg1XX301rrvuOrz00ksAZsY+/jLPPPMM/uZv/gbLli2bUp8p+3n++efjwIEDk18//OEPJ783U/ZxcHAQV155JdLpNL773e/i5Zdfxv/8n/8T7e3tk2N+4+9B8TTl8ssvj9esWTP5/2EYxnPmzInXrVuX4KxOHQDiRx99dPL/oyiKe3p64i9+8YuTtaGhoTibzcb/8A//kMAMTw0HDx6MAcSbN2+O4/jYPqXT6fiRRx6ZHPPKK6/EAOItW7YkNc1TQnt7e/y3f/u3M24fR0dH48WLF8ff+9734t/5nd+JP/WpT8VxPHPO5Z133hlfeOGF9HszZR/jOI7/+I//OL7qqqvM7yfxHjQtPwFVq1Vs374dq1atmqz5vo9Vq1Zhy5YtCc7s7WP37t3o7++fss+FQgErVqw4rfd5eHgYANDR0QEA2L59O2q12pT9XLJkCRYsWHDa7mcYhti4cSNKpRJWrlw54/ZxzZo1eP/73z9lf4CZdS537tyJOXPm4Mwzz8SNN96IPXv2AJhZ+/itb30Ll156KT784Q+jq6sLF198Me6///7J7yfxHjQtF6DDhw8jDEN0d3dPqXd3d6O/vz+hWb29/GK/ZtI+R1GE2267DVdeeSUuuOACAMf2M5PJoFgsThl7Ou7nCy+8gJaWFmSzWXzyk5/Eo48+ivPOO29G7ePGjRvx7LPPYt26dc73Zsp+rlixAg899BAef/xxbNiwAbt378a73vUujI6Ozph9BIDXX38dGzZswOLFi/HEE0/g5ptvxh/+4R/ia1/7GoBk3oOmXTsGMXNYs2YNXnzxxSm/T59JnHvuuXj++ecxPDyM//2//zduuukmbN68OelpnTL6+vrwqU99Ct/73veQy+WSns7bxrXXXjv538uWLcOKFSuwcOFC/NM//RPy+XyCMzu1RFGESy+9FJ///OcBABdffDFefPFF3HfffbjpppsSmdO0/AQ0a9YsBEHgmCYDAwPo6elJaFZvL7/Yr5myz7fccgu+/e1v4wc/+MGUjog9PT2oVqsYGhqaMv503M9MJoOzzz4by5cvx7p163DhhRfir/7qr2bMPm7fvh0HDx7EJZdcglQqhVQqhc2bN+PLX/4yUqkUuru7Z8R+Hk+xWMQ555yDXbt2zZhzCQC9vb0477zzptSWLl06+evGJN6DpuUClMlksHz5cmzatGmyFkURNm3ahJUrVyY4s7ePRYsWoaenZ8o+j4yM4Omnnz6t9jmOY9xyyy149NFH8f3vfx+LFi2a8v3ly5cjnU5P2c8dO3Zgz549p9V+MqIoQqVSmTH7eM011+CFF17A888/P/l16aWX4sYbb5z875mwn8czNjaG1157Db29vTPmXALAlVde6fxJxKuvvoqFCxcCSOg96G1RG04BGzdujLPZbPzQQw/FL7/8cvyJT3wiLhaLcX9/f9JTe8uMjo7Gzz33XPzcc8/FAOK//Mu/jJ977rn4zTffjOM4ju++++64WCzG3/zmN+Of/exn8XXXXRcvWrQonpiYSHjmJ87NN98cFwqF+Mknn4wPHDgw+TU+Pj455pOf/GS8YMGC+Pvf/368bdu2eOXKlfHKlSsTnHXjfOYzn4k3b94c7969O/7Zz34Wf+Yzn4k9z4v/+Z//OY7jmbGPjF+24OJ4Zuznpz/96fjJJ5+Md+/eHf/oRz+KV61aFc+aNSs+ePBgHMczYx/jOI5/8pOfxKlUKv6Lv/iLeOfOnfHXv/71uKmpKf77v//7yTG/6fegabsAxXEcf+UrX4kXLFgQZzKZ+PLLL4+3bt2a9JROih/84AcxAOfrpptuiuP4mAb52c9+Nu7u7o6z2Wx8zTXXxDt27Eh20g3C9g9A/OCDD06OmZiYiP/rf/2vcXt7e9zU1BT/7u/+bnzgwIHkJv0W+C//5b/ECxcujDOZTDx79uz4mmuumVx84nhm7CPj+AVoJuznRz7ykbi3tzfOZDLx3Llz44985CPxrl27Jr8/E/bxFzz22GPxBRdcEGez2XjJkiXxV7/61Snf/02/B6kfkBBCiESYls+AhBBCzHy0AAkhhEgELUBCCCESQQuQEEKIRNACJIQQIhG0AAkhhEgELUBCCCESQQuQEEKIRNACJIQQIhG0AAkhhEgELUBCCCES4f8HYGW83R20XdgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sampling algorithm\n",
    "# for t = T, ... 1 do\n",
    "# z ~ N(0, I) if t > 1, else z =0\n",
    "# x_t-1 = 1/root(alpha_t) * (x_t - (1-alpha_t)/(root(1-alpha_t))*eps_theta(x_t, t)) + sigma_t*z\n",
    "# end for\n",
    "# return x_0\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_current = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "    for i in range(T, 0, -1):\n",
    "        z = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device) if i > 1 else torch.zeros(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "        epsilon_theta = model(x_current, torch.tensor([i - 1], device=device))\n",
    "        x_current = sqrt_recip_alphas[i - 1] * (x_current - (((1 - alphas[i - 1])/(sqrt_one_minus_alphas_cumprod[i-1]))*epsilon_theta)) + (posterior_variance[i-1] * z)\n",
    "        x_current = torch.clamp(x_current, -1.0, 1.0)\n",
    "\n",
    "    show_tensor_image(x_current.cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-3.9.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
